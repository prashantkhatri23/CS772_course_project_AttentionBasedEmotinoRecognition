{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "collapsed_sections": [
        "T_k_HylAmDpK",
        "CQcObV4sapQ_"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Loading Libraries and Data "
      ],
      "metadata": {
        "id": "T_k_HylAmDpK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kkB8KqwObRzw",
        "outputId": "ecc11157-6ba0-4a68-9e48-0dfe26ba0440"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# UPDATE THE MAIN DIRECTORY LOCATION ACCORDINGLY\n",
        "main_dir = '/content/drive/MyDrive/IITB_EE/CS772_project/'"
      ],
      "metadata": {
        "id": "5___h7LSmg5G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import Audio, display\n",
        "import os\n",
        "import pandas as pd\n",
        "import re\n",
        "import os\n",
        "# import opensmile\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchaudio\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import pickle\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch.nn.functional as F\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "import random\n",
        "# Set the random seed\n",
        "random.seed(42)"
      ],
      "metadata": {
        "id": "mGLUMOE7F3YV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !unzip '/content/drive/MyDrive/IITB_EE/CS772_project/IEMOCAP.zip'\n",
        "# # Change directory to the root folder\n",
        "# os.chdir('/content/IEMOCAP')\n",
        "# # List the contents of the root folder\n",
        "# !ls"
      ],
      "metadata": {
        "id": "UpPTNT8lGC0c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # save the DataFrame to a pickle file\n",
        "# data.to_pickle('data_processed.pickle')\n",
        "data_pickel_path = os.path.join(main_dir,'data_processed.pickle')\n",
        "data = pd.read_pickle(data_pickel_path)\n",
        "data.shape"
      ],
      "metadata": {
        "id": "gHn_K3YpcBZL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = data[(data['trans_words'] != '<s> ++GARBAGE++ </s>') & (data['trans_words'] != '<s> ++BREATHING++ </s>')].reset_index(drop=True)\n",
        "data.shape"
      ],
      "metadata": {
        "id": "oIgBkYSdBJN9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **BLSTM Based Unimodal Models**\n",
        "\n",
        "**Note: Same models will be used for the lexical only and audio only classification**\n",
        "\n"
      ],
      "metadata": {
        "id": "CQcObV4sapQ_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Model I -output of the final BLSTM block**"
      ],
      "metadata": {
        "id": "svFMr9GllLmL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# use the output of the final BLSTM  block for the classification\n",
        "class BLSTM_lastblock(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim,num_layers, num_classes):\n",
        "        super().__init__()\n",
        "        self.rnn = nn.LSTM(input_size=input_dim,hidden_size=hidden_dim,num_layers=num_layers,\n",
        "                           batch_first=True, bidirectional=True)\n",
        "        self.fc = nn.Linear(hidden_dim * 2, num_classes)\n",
        "\n",
        "    def forward(self, x,lengths):\n",
        "        # Pack the padded sequence\n",
        "        x = torch.nn.utils.rnn.pack_padded_sequence(x, lengths, batch_first=True, enforce_sorted=False)\n",
        "        # Pass the packed sequence through the LSTM\n",
        "        x, _ = self.rnn(x)\n",
        "        # Unpack the packed sequence\n",
        "        x, _ = torch.nn.utils.rnn.pad_packed_sequence(x, batch_first=True)\n",
        "        # Select the output of the final LSTM block\n",
        "        x = x[torch.arange(x.size(0)), lengths - 1, :]\n",
        "        # Pass the output through the linear layer for classification\n",
        "        x = self.fc(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "-3uyDMG1asDm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Model II -Using averaging pooling**"
      ],
      "metadata": {
        "id": "yKEUYrp4fE91"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# use average pooling of the outputs of the BLSTM block for classification\n",
        "class BLSTM_avg_pooling(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, num_layers, num_classes):\n",
        "        super().__init__()\n",
        "        self.rnn = nn.LSTM(input_size=input_dim, hidden_size=hidden_dim, num_layers=num_layers,\n",
        "                           batch_first=True, bidirectional=True)\n",
        "        self.fc = nn.Linear(hidden_dim * 2, num_classes)\n",
        "\n",
        "    def forward(self, x, lengths):\n",
        "        # Pack the padded sequence\n",
        "        x = torch.nn.utils.rnn.pack_padded_sequence(x, lengths, batch_first=True, enforce_sorted=False)\n",
        "        # Pass the packed sequence through the LSTM\n",
        "        x, _ = self.rnn(x)\n",
        "        # Unpack the packed sequence\n",
        "        x, _ = torch.nn.utils.rnn.pad_packed_sequence(x, batch_first=True)\n",
        "        # Compute the average across the sequence dimension (axis=1)\n",
        "        x = torch.mean(x, dim=1)\n",
        "        # Pass the output through the linear layer for classification\n",
        "        x = self.fc(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "pTsWp2Y3e7uw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Model III - Using context based attention pooling**"
      ],
      "metadata": {
        "id": "92QNXWbffK51"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ContextBasedAttention(nn.Module):\n",
        "    def __init__(self, hidden_dim, attention_dim):\n",
        "        super().__init__()\n",
        "        self.wh = nn.Linear(hidden_dim, attention_dim)\n",
        "        self.v = nn.Parameter(torch.rand(attention_dim, 1))\n",
        "        self.tanh = nn.Tanh()\n",
        "        self.softmax = nn.Softmax(dim=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        hi = self.wh(x)\n",
        "        ei = self.tanh(hi).matmul(self.v)\n",
        "        ai = self.softmax(ei)\n",
        "        z = torch.sum(ai * x, dim=1)\n",
        "        return z\n",
        "\n",
        "class BLSTMWithContextBasedAttention(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, num_layers, attention_dim, num_classes):\n",
        "        super().__init__()\n",
        "        self.rnn = nn.LSTM(input_size=input_dim, hidden_size=hidden_dim, num_layers=num_layers,\n",
        "                           batch_first=True, bidirectional=True)\n",
        "        self.attention = ContextBasedAttention(hidden_dim * 2, attention_dim)\n",
        "        self.fc = nn.Linear(hidden_dim * 2, num_classes)\n",
        "\n",
        "    def forward(self, x, lengths):\n",
        "        # Pack the padded sequence\n",
        "        x = torch.nn.utils.rnn.pack_padded_sequence(x, lengths, batch_first=True, enforce_sorted=False)\n",
        "        # Pass the packed sequence through the LSTM\n",
        "        x, _ = self.rnn(x)\n",
        "        # Unpack the packed sequence\n",
        "        x, _ = torch.nn.utils.rnn.pad_packed_sequence(x, batch_first=True)\n",
        "        # Compute the attention based weighted average across the sequence dimension (axis=1)\n",
        "        x = self.attention(x)\n",
        "        x = self.fc(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "ZieppZjsfH8x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Acousitc Modality"
      ],
      "metadata": {
        "id": "CDGuXRdOF5JF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Audio Dataset Class"
      ],
      "metadata": {
        "id": "sMQEeTmRpuNu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class IEMOCAP_audio(Dataset):\n",
        "\n",
        "    def __init__(self, mean=None, std=None):\n",
        "        # Initialize data, download etc.\n",
        "        data = pd.read_pickle(data_pickel_path)\n",
        "        self.x = [np.array(samp_feat) for samp_feat in data['features']]\n",
        "        y = data['emotion'].values\n",
        "        self.label_encoder = LabelEncoder()\n",
        "        self.spk = data['spk'] \n",
        "        self.y = self.label_encoder.fit_transform(y)\n",
        "        self.n_samples = data.shape[0]\n",
        "        self.mean = mean\n",
        "        self.std = std\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        x_ = torch.tensor(self.x [index])\n",
        "        if self.mean is not None and self.std is not None:\n",
        "            x_ = (x_ - self.mean) / self.std\n",
        "        seq_size = x_.shape[0]\n",
        "        return x_, torch.tensor(self.y[index]), seq_size\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.n_samples\n",
        "\n",
        "    def get_spk(self):\n",
        "        return self.spk\n",
        "        \n",
        "    def get_encoder(self):\n",
        "        return self.label_encoder"
      ],
      "metadata": {
        "id": "2smUbXgik90b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "iemocap_dataset  = IEMOCAP_audio()\n",
        "index = 400\n",
        "print('label = ',iemocap_dataset[index][1])\n",
        "print('sample feature_shape = ',iemocap_dataset [index][0].shape)\n",
        "print('original sequence length = ',iemocap_dataset [index][2])\n",
        "print('inverse label transform',iemocap_dataset.get_encoder().inverse_transform([0, 1, 2,3]))\n",
        "iemocap_dataset[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FW_J410DV_nb",
        "outputId": "5f3c0827-9650-49dd-b488-250e6c9d0329"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "label =  tensor(2)\n",
            "sample feature_shape =  torch.Size([690, 65])\n",
            "original sequence length =  690\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Custom Collate Function**"
      ],
      "metadata": {
        "id": "0xxq_1WTkAqV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def custom_collate_fn(batch):\n",
        "    features, labels,seq_lengths = zip(*batch)\n",
        "    # seq_lengths = [len(seq) for seq in features]\n",
        "\n",
        "    # Sort sequences by length in descending order\n",
        "    seq_lengths, perm_idx = torch.tensor(seq_lengths).sort(0, descending=True)\n",
        "    features = [features[i] for i in perm_idx]\n",
        "    labels = torch.tensor([labels[i] for i in perm_idx])\n",
        "\n",
        "    # Pad the sequences\n",
        "    padded_features = pad_sequence(features, batch_first=True)\n",
        "    return padded_features, labels, seq_lengths"
      ],
      "metadata": {
        "id": "ElvNbxo9nNm5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Randomly splitting the dataset and storing the test (20%) and train (80%) indices**"
      ],
      "metadata": {
        "id": "eVi5LwP3kTYA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "num_samp = len(iemocap_dataset)\n",
        "train_size = int(0.8 * len(iemocap_dataset))\n",
        "test_size = len(iemocap_dataset) - train_size\n",
        "train_indices = random.sample(range(num_samp), train_size)\n",
        "test_indices = [i for i in range(num_samp) if i not in train_indices ]\n",
        "print(\"train_indices\",train_indices)\n",
        "print(\"test_indices\",test_indices)"
      ],
      "metadata": {
        "id": "ERnfhY4OkR7e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 32\n",
        "train_dataset_unnormalized = torch.utils.data.Subset(iemocap_dataset, train_indices)\n",
        "train_loader_unnormalized = DataLoader(dataset=train_dataset_unnormalized, batch_size=batch_size, shuffle=True,collate_fn=custom_collate_fn)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r_a7AhsGlAV6",
        "outputId": "68aa504f-7296-4372-834a-93cc8775f38e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[5238, 912, 204, 2253, 2006, 1828, 1143, 839, 4467, 712, 4837, 3456, 260, 244, 767, 1791, 1905, 4139, 4931, 217, 4597, 1628, 5323, 4464, 3436, 1805, 3679, 4827, 2278, 53, 1307, 3462, 2787, 2276, 1273, 1763, 2757, 837, 759, 3112, 792, 2940, 2817, 4945, 2166, 355, 3763, 4392, 1022, 3100, 645, 4522, 2401, 5149, 5066, 2962, 4729, 1575, 569, 375, 5417, 1866, 2370, 653, 1907, 827, 3113, 2277, 3714, 5207, 2988, 1332, 3032, 2910, 1716, 2187, 5308, 584, 4990, 5201, 1401, 4375, 2005, 1338, 3786, 3108, 2211, 5242, 4562, 1799, 2656, 458, 1876, 262, 2584, 3286, 2193, 542, 1728, 4646, 2577, 1741, 5369, 4089, 3241, 5266, 3758, 1170, 2169, 5513, 2020, 4598, 4415, 2152, 4788, 3509, 4780, 3271, 2965, 1796, 1133, 4174, 4042, 744, 385, 898, 1252, 5140, 1310, 3458, 4885, 520, 3152, 3126, 4881, 3834, 4334, 2059, 4532, 94, 938, 4398, 2185, 5250, 2786, 913, 2404, 3561, 1295, 3716, 26, 2157, 4100, 1463, 4158, 871, 5122, 2444, 5234, 5365, 4988, 1629, 5393, 3063, 1323, 4418, 4344, 4, 4906, 2655, 4002, 159, 916, 2973, 2519, 1961, 474, 1973, 4647, 5469, 701, 3981, 566, 4363, 1030, 1051, 3893, 4503, 1352, 2171, 4322, 4969, 3466, 1735, 4417, 1647, 2553, 3268, 3059, 3588, 4239, 3698, 991, 2030, 1840, 524, 2769, 172, 4819, 4537, 1885, 4820, 1804, 58, 581, 5169, 482, 1875, 552, 257, 2706, 580, 4211, 1949, 2281, 3976, 1755, 5325, 1083, 4677, 4720, 3872, 1990, 3874, 3334, 1559, 772, 794, 3531, 2902, 3469, 3367, 3825, 443, 806, 496, 3298, 2779, 895, 2036, 1569, 1558, 4393, 3675, 1148, 5508, 1503, 5295, 3789, 2046, 617, 3630, 4508, 802, 414, 4428, 120, 764, 1936, 1362, 3329, 3978, 3943, 1751, 3285, 480, 1348, 3104, 17, 3198, 2172, 3727, 2336, 3465, 4552, 3986, 1268, 1555, 2430, 1783, 479, 4744, 4441, 499, 2569, 468, 410, 4785, 3905, 4119, 4350, 1289, 465, 4160, 656, 1522, 561, 4874, 556, 1926, 3307, 982, 4666, 2016, 4742, 4870, 325, 5073, 671, 3434, 4781, 4630, 4282, 2591, 2136, 1673, 2573, 1955, 2175, 3242, 1072, 2457, 3745, 2590, 594, 76, 3754, 5088, 4612, 819, 600, 4404, 1746, 4144, 5239, 1085, 2859, 563, 2001, 3027, 2334, 1292, 3589, 4450, 2478, 5010, 4333, 64, 4543, 2452, 848, 1100, 5475, 945, 876, 5381, 5485, 2231, 2308, 4954, 1725, 2808, 1667, 2162, 4140, 5349, 2057, 416, 756, 5279, 2266, 361, 29, 2732, 1071, 2145, 5355, 3619, 4519, 3503, 4594, 79, 5347, 616, 1221, 4469, 295, 3024, 4771, 4526, 1213, 3520, 1044, 342, 2525, 2987, 326, 2931, 1720, 2044, 842, 2897, 4586, 5249, 5084, 1266, 1939, 1331, 1450, 3377, 203, 1469, 2721, 3372, 2032, 5377, 1304, 885, 3133, 317, 3855, 1822, 1634, 3770, 2864, 2500, 1864, 1826, 193, 1582, 3264, 2689, 2282, 568, 2286, 2876, 4173, 3274, 5472, 2712, 226, 944, 2139, 1462, 4756, 2174, 313, 888, 4887, 3559, 2831, 5225, 3574, 4966, 4189, 947, 3155, 4723, 1557, 2086, 363, 3572, 13, 4259, 4410, 1614, 2983, 3533, 573, 2704, 2571, 1020, 2460, 4154, 2533, 3345, 2672, 3296, 2422, 4541, 1042, 1571, 3444, 3105, 1425, 4662, 2465, 3326, 4488, 3, 2489, 2350, 1721, 3521, 4751, 5328, 2639, 3809, 5132, 3622, 1750, 4187, 3876, 1390, 694, 2324, 4222, 2745, 765, 1924, 2542, 5315, 1631, 1207, 200, 378, 5437, 3892, 596, 3730, 3395, 4715, 1592, 3145, 4049, 3273, 1998, 1208, 45, 873, 3482, 1792, 1440, 4243, 3805, 411, 4566, 2041, 994, 3739, 1092, 3806, 4351, 4578, 4877, 2599, 3625, 4135, 3495, 5021, 3652, 1303, 3888, 3686, 2123, 2025, 2271, 4270, 3969, 1959, 2249, 3603, 634, 2340, 1920, 2225, 2751, 2619, 4424, 660, 5399, 1235, 1894, 3137, 1251, 1752, 526, 3398, 3339, 2710, 4445, 3816, 3406, 510, 1694, 3441, 3190, 4784, 160, 4716, 3116, 3907, 48, 2881, 2446, 3194, 3432, 4409, 4473, 1806, 3999, 1797, 2235, 3570, 5248, 237, 3185, 2753, 3312, 5331, 3828, 1045, 5438, 220, 3227, 4848, 4623, 222, 687, 3511, 1111, 3782, 1488, 4974, 2131, 5026, 2681, 1733, 3724, 2677, 2764, 4884, 2279, 3453, 2066, 670, 3852, 158, 5354, 426, 2866, 1836, 562, 329, 254, 5150, 1633, 166, 1248, 1954, 1034, 3879, 937, 4620, 1785, 5012, 2099, 3021, 1374, 5379, 1341, 2548, 5092, 4740, 210, 2555, 4920, 3074, 3249, 1624, 622, 1989, 834, 2470, 5317, 4636, 336, 2844, 4364, 5404, 3035, 564, 5176, 2795, 103, 4924, 4015, 864, 3551, 2967, 3766, 1253, 3567, 1442, 4274, 2212, 4408, 3960, 3808, 3568, 2198, 2640, 2011, 709, 2284, 3692, 1997, 4968, 4668, 5432, 2755, 235, 4985, 2662, 1489, 3994, 1737, 2906, 2116, 2788, 2290, 2263, 4553, 83, 4232, 1565, 5339, 1977, 5106, 5144, 4548, 1968, 3900, 4020, 3671, 141, 762, 2410, 1815, 4901, 1993, 2508, 4764, 3023, 5007, 4534, 4349, 2816, 3485, 5257, 2709, 2882, 3717, 2219, 2511, 5382, 1888, 988, 1577, 5425, 979, 4389, 1516, 5269, 1772, 3966, 2265, 4297, 2318, 823, 1590, 2426, 1863, 2956, 5098, 2476, 115, 4897, 1036, 2247, 372, 446, 4533, 2393, 4859, 4021, 840, 100, 4702, 2329, 3845, 3921, 3608, 2791, 1510, 420, 2068, 3913, 934, 535, 3282, 4028, 606, 439, 1242, 1222, 4610, 5019, 697, 2033, 970, 4571, 3409, 1848, 4280, 4919, 3690, 3626, 2435, 3512, 2501, 4658, 493, 812, 1702, 5421, 2167, 665, 1286, 1964, 1423, 4521, 614, 1282, 21, 3346, 4693, 3849, 2385, 267, 1896, 2360, 2316, 3719, 583, 1912, 4683, 1620, 4979, 940, 4461, 1841, 1220, 2176, 1165, 5442, 488, 1359, 5345, 2364, 3597, 1018, 3839, 2491, 3297, 2230, 4099, 4423, 4045, 3586, 658, 5113, 3539, 4808, 2051, 211, 748, 5302, 169, 2207, 4866, 1435, 3854, 4251, 5164, 5491, 1486, 5351, 4708, 747, 3850, 2850, 5034, 2730, 2630, 856, 1317, 2701, 5096, 4058, 2361, 3280, 4506, 300, 3725, 721, 2576, 2067, 2648, 949, 3311, 4215, 9, 4444, 3784, 3385, 444, 1536, 4247, 2963, 4083, 3621, 422, 5147, 5444, 4499, 1073, 2359, 5167, 3970, 4971, 236, 1960, 1297, 2545, 4512, 112, 4524, 3342, 763, 4998, 929, 3780, 962, 1261, 4082, 2390, 4168, 2239, 3403, 3952, 3868, 1996, 3741, 4515, 1184, 3142, 1561, 4163, 1118, 571, 5405, 3399, 2784, 4159, 2188, 4675, 2317, 2445, 4011, 1217, 3658, 4412, 3967, 2827, 2723, 4678, 4451, 3090, 5441, 2636, 1545, 1956, 4936, 1913, 3365, 357, 2606, 5286, 3123, 3162, 1246, 4057, 303, 4725, 4114, 2719, 822, 3606, 816, 4308, 3743, 125, 1180, 3358, 1264, 612, 3846, 5330, 2773, 3256, 657, 2691, 4371, 5453, 2594, 3997, 4432, 294, 560, 1923, 2354, 4737, 740, 3555, 5275, 4846, 3634, 5376, 2453, 4904, 376, 2657, 459, 2403, 2936, 3070, 3528, 1192, 2000, 4967, 3375, 1475, 1392, 1434, 646, 5091, 1972, 4076, 1172, 1902, 3777, 2080, 3764, 2091, 5184, 3811, 2356, 1294, 605, 3618, 2830, 2450, 3475, 2048, 3742, 2474, 4997, 3151, 3958, 4980, 1943, 3124, 5478, 2423, 2418, 179, 5190, 2248, 66, 401, 4069, 2344, 5309, 2886, 1794, 5051, 2053, 1120, 795, 322, 2530, 3611, 273, 5449, 1076, 738, 2417, 2676, 4560, 1438, 1644, 1082, 2997, 4348, 4110, 2232, 1347, 2105, 3947, 5310, 2774, 943, 3836, 5126, 1153, 4696, 3255, 2996, 739, 3232, 114, 5157, 1012, 4605, 3019, 2147, 3121, 3043, 887, 1915, 3862, 205, 2687, 1813, 517, 3803, 2475, 3344, 955, 1145, 371, 304, 2493, 4035, 951, 796, 4489, 4889, 3183, 5370, 3039, 3433, 1265, 4932, 811, 4008, 3343, 2291, 268, 4829, 1779, 3632, 3642, 1934, 2971, 813, 3009, 2938, 5274, 3261, 2260, 1554, 1000, 4385, 750, 4793, 174, 5255, 5136, 1995, 1031, 1681, 4867, 1697, 1769, 1908, 4497, 4982, 23, 4953, 1185, 1064, 4422, 1429, 900, 4634, 1079, 121, 2934, 5296, 2652, 129, 1427, 2173, 429, 1038, 3448, 931, 5388, 3901, 3672, 5401, 4204, 893, 3702, 4127, 1814, 5474, 4271, 4836, 3752, 255, 498, 3923, 3290, 3492, 884, 4016, 3633, 602, 661, 2638, 1215, 538, 1033, 2252, 2663, 3120, 2415, 4359, 4141, 3524, 4686, 4857, 1761, 3523, 3699, 1871, 3389, 2776, 3715, 3266, 3407, 778, 2560, 3496, 4254, 2088, 3066, 1250, 3885, 549, 4856, 699, 4570, 3537, 791, 3052, 1065, 491, 2700, 1001, 4572, 2896, 3464, 421, 4452, 2559, 2880, 5159, 4156, 1742, 1267, 3950, 1837, 886, 2868, 3011, 941, 5077, 1852, 3515, 215, 2190, 4479, 1477, 2238, 2531, 2783, 2875, 50, 4893, 1173, 3283, 570, 1162, 251, 751, 1762, 3081, 3439, 4269, 2792, 5218, 3031, 2552, 4477, 695, 430, 1274, 4195, 407, 668, 2229, 3629, 3473, 4905, 4588, 3392, 2237, 1765, 932, 4535, 5016, 908, 2320, 5361, 2526, 5463, 4910, 3237, 448, 62, 1674, 2469, 1730, 1124, 2093, 2371, 4376, 5208, 63, 4074, 3527, 1439, 1058, 3114, 4426, 4098, 2901, 590, 3252, 346, 3573, 153, 5311, 637, 2564, 3516, 3313, 3421, 5216, 4397, 3318, 170, 2660, 1407, 3769, 2964, 4604, 3577, 867, 4769, 3569, 4608, 644, 4492, 2541, 2781, 5121, 2728, 1377, 625, 4711, 1588, 2862, 5074, 1209, 1935, 5109, 1199, 2096, 1616, 1421, 5357, 5259, 1451, 4047, 3800, 3677, 2647, 2589, 1236, 3602, 279, 5476, 1811, 2586, 1240, 4339, 1125, 5031, 230, 1441, 2078, 4516, 1271, 1891, 1851, 154, 233, 5359, 4927, 1175, 314, 4834, 4637, 4003, 5130, 369, 4651, 2433, 2076, 1574, 1895, 2377, 2270, 3908, 3243, 3026, 3669, 168, 1842, 3723, 3317, 2341, 2669, 771, 1316, 5165, 1948, 4316, 4103, 3922, 253, 1845, 423, 3321, 3682, 3429, 1406, 4023, 2925, 345, 4875, 2646, 706, 4921, 1014, 2898, 1793, 5430, 2146, 2141, 2497, 650, 1490, 1527, 3759, 1158, 1586, 4165, 3172, 1385, 2780, 2448, 214, 4748, 4750, 2651, 1370, 269, 1350, 387, 2285, 2778, 1583, 1163, 1032, 4130, 3488, 5078, 4225, 3712, 2468, 3576, 615, 1365, 333, 2386, 2718, 579, 5183, 1432, 1270, 3963, 4545, 2860, 5070, 1605, 528, 2437, 2903, 3842, 347, 5233, 2289, 1542, 2635, 4122, 1345, 3330, 523, 2744, 2878, 4989, 4858, 4059, 2807, 3737, 2156, 382, 4033, 2746, 1734, 2082, 1482, 74, 1485, 4356, 4413, 3895, 877, 1399, 3881, 3138, 1991, 786, 927, 5172, 4947, 316, 1211, 5390, 3228, 4331, 2079, 3157, 2210, 3420, 3025, 5011, 4440, 4079, 2711, 1379, 4581, 5247, 5297, 536, 4915, 1543, 631, 664, 4486, 3405, 2837, 3158, 2558, 5502, 3697, 678, 2953, 4978, 178, 1682, 1492, 2770, 2947, 972, 1819, 5105, 1167, 4199, 4250, 3210, 1838, 958, 4585, 4749, 4226, 4770, 3212, 1921, 4294, 3929, 5282, 1506, 4071, 3962, 5237, 5307, 4081, 5227, 1154, 3187, 1564, 4754, 2160, 1714, 663, 4617, 817, 3281, 4734, 5337, 3575, 1024, 213, 2626, 4778, 3580, 1520, 4319, 3826, 4714, 2913, 4234, 2113, 3486, 4926, 3896, 343, 3125, 5215, 1117, 4569, 3708, 2102, 603, 5045, 5504, 5460, 3729, 909, 3867, 1847, 3623, 1431, 4453, 109, 1699, 218, 1623, 2056, 1531, 965, 1581, 334, 1535, 919, 4733, 1305, 3812, 405, 3437, 2927, 4796, 1373, 5415, 598, 3939, 156, 4075, 3757, 4355, 5178, 2851, 3404, 5076, 5434, 2889, 4248, 4224, 2520, 5161, 3710, 324, 77, 1048, 883, 3883, 5332, 4730, 2978, 4366, 2161, 4882, 455, 3179, 4607, 973, 1233, 5226, 195, 976, 1719, 2617, 3251, 2551, 1872, 5300, 454, 3427, 3707, 2047, 5362, 2195, 67, 2588, 2110, 2355, 990, 2943, 588, 1193, 1758, 6, 2518, 1445, 985, 2337, 1706, 5505, 5097, 2737, 350, 2144, 4213, 277, 2154, 2228, 4084, 3221, 3932, 2269, 4786, 1599, 4128, 1925, 3904, 2603, 1584, 1529, 4303, 3061, 4430, 5465, 3229, 276, 1412, 987, 3002, 5329, 5193, 424, 3160, 2383, 4221, 3099, 1361, 546, 181, 1443, 2236, 1386, 3332, 2632, 717, 3401, 3191, 2804, 1903, 2848, 4577, 2587, 746, 3323, 5301, 258, 2932, 3779, 1874, 151, 1201, 825, 4433, 3240, 3866, 3628, 171, 5168, 3984, 2111, 4442, 5501, 2224, 5103, 1037, 149, 3085, 2649, 782, 1171, 3721, 5281, 5240, 3774, 3546, 2685, 4652, 1119, 509, 5072, 1507, 1789, 3643, 1638, 3044, 1801, 5403, 1388, 5001, 5095, 2834, 2037, 1504, 3265, 2126, 5014, 3865, 338, 4595, 1738, 323, 5484, 2467, 3368, 4390, 2234, 1203, 1315, 3853, 327, 1343, 2707, 1210, 1255, 5082, 4794, 2939, 1745, 682, 2825, 1818, 5036, 1831, 173, 2977, 3563, 1444, 3750, 1781, 4162, 4942, 4854, 234, 307, 2749, 2611, 1663, 5061, 2101, 3638, 3071, 2782, 655, 127, 4654, 3476, 2488, 2777, 3200, 4941, 142, 4374, 275, 966, 3188, 2642, 1500, 1483, 1568, 2323, 132, 4797, 628, 4120, 4056, 1519, 1523, 3614, 4719, 4228, 5018, 3807, 2168, 5263, 1630, 1287, 4133, 1141, 5471, 464, 106, 3013, 4773, 5110, 2120, 1585, 2300, 5303, 5189, 3174, 1066, 2883, 1827, 878, 2506, 1169, 2842, 4807, 5180, 502, 555, 3493, 4589, 4080, 707, 2921, 1823, 359, 3319, 2793, 3463, 1309, 2735, 4679, 4792, 265, 4273, 2220, 1189, 1228, 3487, 5340, 2915, 5118, 5003, 2612, 713, 3250, 1480, 2083, 918, 497, 4483, 3244, 3799, 969, 5431, 2023, 107, 1478, 3733, 4427, 1511, 1914, 3291, 2259, 531, 3540, 353, 4007, 5104, 3560, 4474, 2944, 3694, 2153, 3903, 3150, 1677, 4611, 4960, 302, 513, 1298, 2631, 3532, 1843, 4323, 2785, 2119, 1411, 525, 3983, 3195, 2258, 2620, 2407, 745, 3144, 4182, 4745, 3871, 5069, 4712, 508, 2122, 626, 1245, 674, 4558, 1321, 2905, 923, 1417, 2125, 4001, 4482, 1026, 804, 2601, 2256, 3605, 512, 4252, 4090, 2516, 5094, 3957, 2058, 2625, 690, 2424, 2378, 3916, 4783, 2698, 2557, 2951, 4145, 1382, 5151, 4517, 3380, 116, 332, 186, 2627, 3911, 2362, 1084, 2667, 863, 3141, 2343, 3746, 3920, 3771, 124, 2039, 2568, 4184, 1186, 2629, 1237, 1978, 1003, 5032, 3306, 2805, 3598, 1218, 1857, 298, 2820, 245, 647, 1800, 4685, 1983, 4459, 835, 1393, 2484, 4172, 1280, 2942, 5489, 3007, 1414, 4863, 4710, 3115, 1517, 2109, 3548, 434, 3631, 3756, 1910, 501, 1095, 4688, 1015, 577, 396, 207, 1188, 1573, 3794, 1712, 1016, 654, 3884, 4849, 2366, 3906, 3370, 777, 3566, 652, 2040, 2108, 4664, 2042, 1263, 4959, 5380, 368, 1610, 2070, 3768, 986, 881, 2389, 3749, 199, 3349, 1152, 2027, 2447, 2679, 2754, 1927, 5412, 4809, 33, 440, 4178, 548, 5291, 2979, 1498, 3128, 1651, 3303, 185, 1640, 209, 2335, 2301, 797, 1484, 5139, 1182, 301, 5080, 2065, 1844, 3130, 3518, 4369, 4193, 5375, 2502, 486, 527, 3350, 1613, 1528, 5071, 3647, 4805, 1497, 3092, 4148, 815, 4347, 2085, 4813, 4929, 164, 3299, 5348, 5213, 2922, 1364, 1940, 2127, 4261, 610, 2483, 5414, 572, 3624, 4755, 5093, 4682, 1609, 5115, 3028, 1225, 2431, 4117, 2077, 2087, 2180, 5515, 5280, 2305, 1227, 1945, 68, 1508, 1356, 2759, 4167, 3400, 4563, 1260, 2970, 2818, 5419, 4899, 3762, 4438, 1087, 3659, 3206, 3687, 2369, 2365, 4177, 2954, 4845, 4895, 1967, 698, 4383, 4603, 3673, 2538, 4281, 4642, 5166, 4790, 992, 4307, 2345, 2869, 450, 781, 3790, 4169, 1284, 1715, 620, 1691, 2828, 3374, 1680, 2055, 3545, 2504, 1931, 3553, 3304, 4287, 2890, 565, 2124, 849, 2297, 5102, 3980, 4950, 5406, 3917, 1285, 4806, 1882, 2183, 1402, 4179, 4102, 2768, 2952, 4640, 2634, 2839, 4407, 2498, 3376, 787, 1009, 1142, 4004, 4704, 921, 1219, 1183, 2887, 4360, 4176, 5264, 2002, 3471, 1965, 1428, 2295, 2955, 4421, 1178, 3775, 2348, 2771, 4298, 5232, 4106, 1413, 599, 5145, 5312, 4463, 4036, 3438, 1418, 4092, 2686, 1050, 4055, 1962, 5155, 5454, 2206, 1110, 2302, 3801, 1112, 4326, 447, 2521, 3034, 4475, 980, 993, 4962, 2743, 5191, 924, 2613, 953, 4216, 5223, 1692, 4900, 4341, 4345, 4886, 2790, 3609, 3793, 2254, 5496, 1666, 2674, 4048, 5192, 2664, 4510, 1176, 3627, 4072, 2645, 242, 366, 2671, 2351, 4831, 2191, 3928, 2961, 152, 3700, 1711, 720, 148, 1626, 2029, 4244, 4318, 5492, 37, 3148, 2326, 3927, 4706, 1372, 4655, 1861, 3859, 4944, 3207, 3497, 4591, 3288, 1917, 3843, 789, 461, 1354, 3248, 2994, 1879, 5350, 1053, 3755, 3552, 57, 3017, 1380, 4321, 2325, 2762, 4242, 718, 3595, 2616, 1660, 5246, 5413, 5086, 4291, 1641, 2741, 390, 5141, 575, 3078, 1325, 4496, 27, 1068, 5028, 964, 1829, 1093, 4006, 4096, 4671, 2960, 2346, 46, 5135, 4880, 1472, 3912, 967, 4865, 3522, 484, 1906, 1256, 4352, 3961, 2810, 3426, 4916, 5153, 4787, 5244, 3018, 5363, 1505, 2517, 904, 897, 547, 1958, 627, 4487, 2481, 1530, 1703, 4209, 4949, 1928, 2201, 2720, 894, 1013, 4709, 2440, 335, 3169, 3000, 2163, 4758, 1481, 319, 2310, 4476, 252, 2243, 3325, 828, 3197, 3310, 613, 673, 3014, 5389, 1809, 476, 3886, 841, 2388, 3140, 4170, 2090, 1824, 227, 3383, 540, 3840, 1701, 4584, 2309, 5056, 2288, 1893, 4799, 1262, 89, 1621, 5029, 12, 892, 2368, 299, 3263, 5326, 3851, 3510, 261, 2216, 246, 282, 1933, 3662, 4975, 4017, 737, 554, 3408, 3072, 5017, 1533, 1566, 3795, 1546, 1538, 328, 2796, 4930, 2208, 544, 4411, 1424, 485, 731, 2200, 3247, 4387, 521, 5518, 14, 93, 4241, 4669, 2758, 4644, 5163, 2178, 1553, 4660, 3135, 1886, 4824, 635, 1129, 5283, 462, 131, 5040, 1717, 3607, 5162, 3748, 844, 3582, 413, 2432, 137, 5514, 4013, 2752, 3526, 1002, 3706, 5324, 4908, 959, 3930, 889, 231, 5041, 2063, 4554, 2833, 4582, 1306, 3214, 2449, 3909, 1313, 4699, 4406, 586, 3798, 2135, 905, 1693, 1230, 1127, 249, 2280, 4435, 3062, 4268, 4014, 1749, 5486, 5316, 192, 1410, 4580, 3600, 5235, 2148, 4928, 1705, 1672, 611, 4217, 1541, 753, 2203, 4775, 4757, 3442, 1231, 591, 4869, 2917, 2303, 1690, 3934, 4219, 2170, 3041, 4739, 4579, 5117, 832, 4531, 4601, 4290, 4622, 4125, 4994, 5423, 780, 4313, 1099, 1544, 3230, 2477, 4164, 161, 3382, 770, 2406, 2750, 3972, 5062, 1963, 855, 1363, 1241, 4166, 4293, 4255, 2975, 5344, 5470, 4372, 5482, 4840, 3101, 2261, 4613, 1159, 5284, 1909, 3428, 4192, 1471, 4536, 2992, 1473, 2100, 5270, 3467, 403, 4362, 1311, 5424, 1518, 1281, 1695, 187, 2304, 906, 597, 3722, 3003, 3292, 2392, 2372, 1710, 4111, 624, 803, 1351, 942, 3302, 2333, 1006, 2045, 2255, 4883, 3415, 1054, 5171, 2628, 2009, 1887, 689, 4816, 693, 2829, 4212, 4470, 2789, 3517, 2690, 240, 4026, 138, 4367, 198, 5459, 1689, 3668, 2583, 5054, 278, 618, 3053, 5271, 2071, 1867, 3276, 247, 4871, 4830, 3246, 1979, 4983, 3695, 3004, 2179, 3489, 1684, 4917, 3233, 3562, 4088, 2194, 1174, 70, 5143, 4695, 734, 438, 394, 4381, 609, 5112, 3941, 3204, 3926, 4815, 3161, 1094, 1625, 3995, 3829, 5129, 1880, 2314, 996, 925, 4843, 330, 4235, 3765, 2981, 2861, 2760, 4779, 4309, 229, 2608, 38, 3787, 3637, 5473, 1777, 3231, 4542, 3010, 2949, 4332, 4220, 865, 2321, 1947, 3110, 190, 2675, 5265, 3129, 3414, 134, 2641, 1288, 4194, 4229, 5373, 3270, 3674, 315, 3501, 3103, 2918, 984, 2598, 1593, 2214, 1358, 1572, 2684, 321, 4635, 1416, 219, 400, 1790, 950, 311, 1395, 3416, 2514, 2439, 4760, 1336, 4311, 3657, 4043, 4493, 4115, 1457, 4743, 1537, 1768, 760, 2399, 1556, 351, 5035, 4762, 3875, 4284, 4833, 1097, 4138, 2546, 3931, 1595, 4576, 3740, 436, 374, 3084, 4873, 1821, 1474, 1104, 418, 5422, 4523, 5187, 5209, 4992, 5262, 2272, 2098, 1671, 5397, 4253, 367, 1449, 4062, 381, 5030, 2822, 1328, 5015, 4208, 1195, 4925, 4514, 4146, 5124, 4500, 2342, 2134, 5005, 4330, 2496, 3500, 1101, 3776, 2761, 2019, 3924, 259, 684, 1780, 1130, 1724, 3235, 3153, 2582, 1479, 5335, 1010, 2802, 2843, 800, 4632, 4073, 2892, 1247, 2654, 5024, 1360, 2537, 1349, 1803, 1371, 3579, 2434, 557, 2812, 3381, 4574, 4152, 4681, 727, 2670, 4505, 3753, 4976, 2358, 3199, 1334, 1540, 2593, 2873, 1495, 466, 2579, 818, 2425, 3394, 736, 5516, 110, 1890, 858, 1795, 2451, 283, 1675, 2696, 4296, 3430, 4835, 4555, 2536, 3781, 2615, 4183, 1839, 2456, 2384, 3012, 4544, 2075, 1713, 2226, 150, 2990, 2199, 348, 4878, 3005, 1454, 681, 2836, 2306, 1636, 2574, 2462, 433, 43, 3744, 1059, 5384, 4455, 3175, 3149, 4063, 5222, 2814, 1825, 1532, 1602, 3593, 2049, 4046, 3718, 4722, 4782, 428, 4625, 5464, 4575, 475, 5298, 1426, 1277, 1405, 1865, 847, 4024, 3390, 5299, 1950, 399, 3585, 1853, 2824, 2845, 1229, 183, 471, 92, 5457, 3935, 4661, 999, 4619, 714, 4338, 3592, 1355, 2293, 5177, 1892, 4600, 1659, 755, 3056, 1770, 3050, 119, 808, 3353, 1759, 4987, 18, 879, 4109, 1140, 256, 5142, 2409, 3646, 3300, 1335, 807, 4956, 2442, 1074, 2004, 4888, 1283, 1591, 1607, 467, 5025, 4085, 2473, 4032, 3384, 3974, 3387, 4676, 3547, 977, 4249, 5229, 749, 4335, 3987, 2092, 2624, 2413, 4828, 4197, 1732, 4894, 4126, 2958, 3166, 2849, 683, 4993, 4986, 574, 766, 4776, 3832, 5488, 1470, 39, 1984, 559, 799, 5451, 2941, 3316, 2429, 2014, 5100, 4817, 4258, 685, 3599, 126, 903, 1196, 133, 4150, 920, 2202, 3490, 688, 1873, 2028, 2251, 2296, 463, 2312, 2539, 2592, 1502, 2680, 4134, 1807, 5131, 2665, 5418, 419, 5173, 3146, 5057, 2767, 4414, 4306, 356, 1798, 2742, 2819, 859, 1436, 1753, 679, 2427, 846, 2907, 243, 2412, 3239, 1151, 3111, 3982, 3006, 1198, 3055, 3354, 2107, 5519, 3534, 1685, 3594, 1135, 519, 543, 3556, 3336, 1319, 2241, 1608, 2604, 582, 5158, 1696, 4592, 3663, 309, 2510, 1650, 3253, 1139, 4283, 1862, 1999, 1340, 10, 5408, 1877, 1467, 2487, 5358, 2908, 3419, 3965, 2026, 2490, 1322, 1161, 1383, 534, 1987, 4132, 3080, 188, 5008, 3165, 3636, 3450, 3379, 2242, 1869, 2, 4934, 4436, 3260, 3224, 651, 1122, 2618, 1476, 1025, 337, 4810, 677, 212, 5277, 4935, 2826, 4288, 373, 389, 5352, 874, 1953, 2946, 4256, 926, 3257, 5182, 2610, 3309, 2872, 4687, 1729, 541, 4527, 290, 4265, 4746, 365, 2683, 4587, 2315, 2972, 2581, 5416, 1567, 545, 585, 280, 34, 672, 1802, 4912, 872, 3189, 1686, 1812, 5059, 3180, 3087, 2572, 504, 5075, 3613, 5067, 2809, 4147, 4030, 1134, 4365, 4902, 3847, 3222, 4938, 2930, 2854, 1604, 5333, 223, 2736, 3507, 1580, 3036, 4529, 5260, 2121, 49, 2479, 1296, 3479, 3726, 1723, 1496, 3530, 1929, 2914, 3667, 5450, 56, 4583, 3788, 163, 2661, 3483, 4206, 2920, 3954, 2835, 2991, 4116, 3783, 3678, 3938, 2756, 1047, 2858, 3051, 4329, 1654, 318, 1098, 669, 4443, 3684, 1299, 1563, 1709, 1330, 1394, 2734, 135, 2856, 1430, 2240, 1494, 1952, 4087, 4653, 118, 500, 176, 4690, 4853, 4214, 928, 4818, 4518, 4573, 633, 3259, 2717, 4402, 4747, 4155, 5506, 25, 2512, 281, 5065, 4320, 3159, 5050, 1951, 710, 2726, 2395, 1603, 4009, 2948, 530, 341, 761, 4113, 3819, 3731, 1788, 550, 1501, 5220, 241, 1579, 2438, 60, 686, 1652, 2330, 5037, 3660, 130, 4275, 1458, 3778, 3167, 157, 3396, 3213, 1046, 2893, 113, 1023, 3097, 1160, 1986, 2694, 4419, 522, 3648, 3211, 3612, 2895, 3065, 3506, 3571, 4431, 3362, 5446, 5, 90, 2250, 1767, 4131, 2017, 3328, 3581, 3457, 784, 1884, 3601, 4911, 5455, 1756, 72, 1132, 4257, 2919, 3664, 2197, 1342, 4067, 3481, 659, 3870, 852, 2441, 5511, 2347, 4230, 3857, 3262, 4599, 2339, 2928, 1028, 1378, 4965, 2561, 2513, 457, 3711, 1056, 3088, 821, 4774, 5400, 952, 1773, 578, 4034, 4551, 636, 5212, 5206, 5000, 935, 16, 4946, 5402, 5320, 4727, 1606, 4839, 1514, 1275, 4340, 2565, 754, 3936, 2865, 3289, 1589, 2567, 3578, 3287, 2556, 3202, 4101, 3535, 308, 5047, 5512, 2650, 477, 1109, 2287, 1302, 3772, 1420, 2015, 19, 5342, 2633, 4337, 1346, 2821, 3635, 2186, 4795, 629, 2143, 4692, 5174, 487, 2227, 1007, 122, 291, 4205, 4107, 2595, 2912, 5343, 1731, 4317, 845, 4628, 3245, 4850, 3823, 5292, 4437, 3661, 1035, 4416, 4826, 1679, 3223, 3054, 1212, 4203, 3591, 890, 2605, 4670, 2985, 5447, 5313, 4227, 3513, 5273, 5495, 2129, 2376, 3944, 4186, 2151, 3940, 5211, 478, 5356, 415, 2668, 1597, 3514, 1197, 3736, 5199, 216, 2540, 2257, 954, 643, 3988, 2054, 3785, 733, 1784, 425, 2622, 5148, 948, 2524, 3617, 3412, 3443, 2021, 1653, 3474, 623, 3091, 3705, 4403, 3505, 82, 3308, 3363, 5440, 5392, 4697, 1456, 4379, 728, 4354, 5334, 2380, 3703, 5435, 532, 2724, 5152, 4157, 3738, 4310, 1138, 3989, 1126, 511, 5081, 946, 208, 1646, 4180, 286, 248, 24, 2262, 1700, 1982, 4567, 3877, 3814, 3596, 820, 2528, 1760, 1290, 1320, 4520, 4471, 1200, 4663, 1664, 1810, 44, 4907, 404, 4501, 5398, 1657, 263, 2164, 1656, 2246, 250, 87, 5068, 4825, 344, 2597, 4380, 3478, 4530, 3388, 4434, 1974, 783, 3945, 4439, 4672, 724, 3049, 383, 3069, 2950, 5374, 4494, 2644, 1257, 3973, 2727, 503, 2575, 604, 1021, 843, 2811, 981, 3397, 4040, 2926, 3181, 2184, 1918, 408, 4050, 1632, 2233, 3848, 1291, 1642, 1113, 1627, 2803, 1687, 1601, 1460, 735, 5360, 1115, 1600, 2840, 704, 4019, 1904, 4639, 4420, 553, 3327, 5490, 5456, 2867, 3720, 1683, 1594, 3008, 3878, 5108, 3451, 732, 1570, 3131, 3688, 4638, 4377, 507, 3968, 3440, 3015, 3639, 1509, 4388, 5288, 449, 1179, 4462, 726, 5353, 360, 2464, 5119, 5221, 5304, 5120, 4097, 1870, 5170, 4977, 4465, 3060, 1808, 1878, 3079, 4394, 5395, 1008, 2596, 632, 2003, 1004, 3193, 184, 3817, 3127, 1232, 3058, 40, 1078, 978, 1, 2715, 1670, 412, 1718, 4701, 1468, 5049, 662, 3418, 1704, 2420, 4700, 1757, 3077, 2924, 1077, 4246, 5321, 5004, 3117, 5494, 810, 1708, 206, 2215, 5367, 4940, 4094, 4200, 1499, 4736, 4263, 997, 1329, 649, 1975, 1448, 1150, 1850, 5043, 730, 1899, 2747, 1206, 2414, 774, 5048, 5372, 2800, 5341, 5507, 1562, 3902, 5020, 1437, 2052, 801, 4384, 4667, 3815, 5420, 4855, 4104, 452, 2550, 1635, 3860, 4838, 288, 4918, 2722, 1835, 4943, 2982, 4502, 4218, 3102, 3470, 3048, 1308, 4891, 914, 4803, 2132, 4304, 2387, 1419, 3964, 4812, 4267, 221, 4698, 1846, 98, 790, 2969, 1834, 340, 2221, 4999, 775, 719, 4550, 768, 3709, 1661, 2283, 1333, 3640, 2074, 3990, 3096, 2411, 2060, 2766, 41, 3335, 3649, 4012, 3685, 700, 2408, 1465, 4629, 2794, 3134, 2841, 692, 1216, 2298, 4752, 2863, 3824, 5436, 3641, 1243, 2268, 5039, 2084, 1464, 1539, 4425, 2137, 838, 4626, 1106, 3996, 4299, 576, 2637, 4124, 196, 101, 3644, 3887, 3460, 1005, 3565, 111, 3948, 7, 875, 4053, 2405, 3033, 3338, 4768, 983, 2038, 2165, 52, 2515, 349, 5160, 4401, 167, 3499, 1944, 2549, 814, 3001, 4237, 1353, 1766, 4061, 1314, 1114, 793, 2765, 5481, 4914, 4342, 2149, 2870, 91, 539, 4314, 4262, 3810, 2995, 1980, 1155, 1722, 3894, 4327, 175, 5467, 1108, 5452, 4996, 5217, 4939, 4373, 5479, 1070, 857, 1096, 3899, 4460, 3386, 975, 4561, 2043, 5198, 71, 5087, 3073, 4153, 3366, 3676, 4121, 2899, 3818, 354, 5116, 2398, 5013, 3689, 2813, 5064, 4031, 891, 4832, 5318, 5268, 2307, 3455, 2443, 3538, 3728, 3882, 3890, 4343, 4446, 667, 3176, 4382, 1820, 5251, 1276, 5493, 4763, 4772, 4086, 4091, 306, 5009, 5278, 1204, 4060, 5044, 2485, 4054, 4528, 3361, 4105, 4481, 3953, 4602, 3827, 2499, 3665, 4289, 968, 4454, 352, 4449, 3683, 4491, 3484, 2010, 5188, 5128, 1547, 2535, 3119, 1743, 4458, 1368, 3869, 1453, 691, 3529, 2688, 3587, 2697, 201, 2319, 3615, 3992, 907, 4498, 551, 853, 1238, 182, 384, 3225, 432, 833, 1637, 1057, 1550, 1576, 1055, 5203, 5186, 4142, 3670, 826, 4970, 2643, 1164, 59, 1643, 5448, 2097, 2018, 2189, 3760, 5497, 2731, 1409, 2602, 2112, 1922, 3324, 3423, 4266, 4044, 5002, 495, 1269, 1387, 5194, 1942, 971, 4616, 2855, 716, 705, 3820, 529, 2547, 4665, 3216, 4852, 5058, 1966, 5060, 2472, 453, 1459, 1397, 4161, 2311, 1744, 1669, 2331, 3959, 270, 2322, 5241, 5428, 516, 2607, 3494, 4909, 4051, 2349, 5338, 2976, 4728, 4370, 3168, 3544, 4264, 4645, 15, 3331, 4068, 1149, 2461, 4196, 4961, 3691, 3858, 3083, 3863, 2580, 567, 1491, 3047, 1889, 2062, 3942, 492, 3411, 5480, 5027, 5466, 1398, 4648, 3201, 3998, 3378, 1736, 5287, 3835, 388, 702, 2192, 4615, 974, 4231, 5385, 5290, 1447, 370, 601, 2838, 3655, 5426, 1525, 4547, 4649, 4447, 1137, 2454, 4724, 5214, 861, 5055, 1985, 533, 4279, 3154, 2379, 963, 2024, 145, 5407, 398, 3861, 1764]\n",
            "[0, 8, 11, 20, 22, 28, 30, 31, 32, 35, 36, 42, 47, 51, 54, 55, 61, 65, 69, 73, 75, 78, 80, 81, 84, 85, 86, 88, 95, 96, 97, 99, 102, 104, 105, 108, 117, 123, 128, 136, 139, 140, 143, 144, 146, 147, 155, 162, 165, 177, 180, 189, 191, 194, 197, 202, 224, 225, 228, 232, 238, 239, 264, 266, 271, 272, 274, 284, 285, 287, 289, 292, 293, 296, 297, 305, 310, 312, 320, 331, 339, 358, 362, 364, 377, 379, 380, 386, 391, 392, 393, 395, 397, 402, 406, 409, 417, 427, 431, 435, 437, 441, 442, 445, 451, 456, 460, 469, 470, 472, 473, 481, 483, 489, 490, 494, 505, 506, 514, 515, 518, 537, 558, 587, 589, 592, 593, 595, 607, 608, 619, 621, 630, 638, 639, 640, 641, 642, 648, 666, 675, 676, 680, 696, 703, 708, 711, 715, 722, 723, 725, 729, 741, 742, 743, 752, 757, 758, 769, 773, 776, 779, 785, 788, 798, 805, 809, 824, 829, 830, 831, 836, 850, 851, 854, 860, 862, 866, 868, 869, 870, 880, 882, 896, 899, 901, 902, 910, 911, 915, 917, 922, 930, 933, 936, 939, 956, 957, 960, 961, 989, 995, 998, 1011, 1017, 1019, 1027, 1029, 1039, 1040, 1041, 1043, 1049, 1052, 1060, 1061, 1062, 1063, 1067, 1069, 1075, 1080, 1081, 1086, 1088, 1089, 1090, 1091, 1102, 1103, 1105, 1107, 1116, 1121, 1123, 1128, 1131, 1136, 1144, 1146, 1147, 1156, 1157, 1166, 1168, 1177, 1181, 1187, 1190, 1191, 1194, 1202, 1205, 1214, 1223, 1224, 1226, 1234, 1239, 1244, 1249, 1254, 1258, 1259, 1272, 1278, 1279, 1293, 1300, 1301, 1312, 1318, 1324, 1326, 1327, 1337, 1339, 1344, 1357, 1366, 1367, 1369, 1375, 1376, 1381, 1384, 1389, 1391, 1396, 1400, 1403, 1404, 1408, 1415, 1422, 1433, 1446, 1452, 1455, 1461, 1466, 1487, 1493, 1512, 1513, 1515, 1521, 1524, 1526, 1534, 1548, 1549, 1551, 1552, 1560, 1578, 1587, 1596, 1598, 1611, 1612, 1615, 1617, 1618, 1619, 1622, 1639, 1645, 1648, 1649, 1655, 1658, 1662, 1665, 1668, 1676, 1678, 1688, 1698, 1707, 1726, 1727, 1739, 1740, 1747, 1748, 1754, 1771, 1774, 1775, 1776, 1778, 1782, 1786, 1787, 1816, 1817, 1830, 1832, 1833, 1849, 1854, 1855, 1856, 1858, 1859, 1860, 1868, 1881, 1883, 1897, 1898, 1900, 1901, 1911, 1916, 1919, 1930, 1932, 1937, 1938, 1941, 1946, 1957, 1969, 1970, 1971, 1976, 1981, 1988, 1992, 1994, 2007, 2008, 2012, 2013, 2022, 2031, 2034, 2035, 2050, 2061, 2064, 2069, 2072, 2073, 2081, 2089, 2094, 2095, 2103, 2104, 2106, 2114, 2115, 2117, 2118, 2128, 2130, 2133, 2138, 2140, 2142, 2150, 2155, 2158, 2159, 2177, 2181, 2182, 2196, 2204, 2205, 2209, 2213, 2217, 2218, 2222, 2223, 2244, 2245, 2264, 2267, 2273, 2274, 2275, 2292, 2294, 2299, 2313, 2327, 2328, 2332, 2338, 2352, 2353, 2357, 2363, 2367, 2373, 2374, 2375, 2381, 2382, 2391, 2394, 2396, 2397, 2400, 2402, 2416, 2419, 2421, 2428, 2436, 2455, 2458, 2459, 2463, 2466, 2471, 2480, 2482, 2486, 2492, 2494, 2495, 2503, 2505, 2507, 2509, 2522, 2523, 2527, 2529, 2532, 2534, 2543, 2544, 2554, 2562, 2563, 2566, 2570, 2578, 2585, 2600, 2609, 2614, 2621, 2623, 2653, 2658, 2659, 2666, 2673, 2678, 2682, 2692, 2693, 2695, 2699, 2702, 2703, 2705, 2708, 2713, 2714, 2716, 2725, 2729, 2733, 2738, 2739, 2740, 2748, 2763, 2772, 2775, 2797, 2798, 2799, 2801, 2806, 2815, 2823, 2832, 2846, 2847, 2852, 2853, 2857, 2871, 2874, 2877, 2879, 2884, 2885, 2888, 2891, 2894, 2900, 2904, 2909, 2911, 2916, 2923, 2929, 2933, 2935, 2937, 2945, 2957, 2959, 2966, 2968, 2974, 2980, 2984, 2986, 2989, 2993, 2998, 2999, 3016, 3020, 3022, 3029, 3030, 3037, 3038, 3040, 3042, 3045, 3046, 3057, 3064, 3067, 3068, 3075, 3076, 3082, 3086, 3089, 3093, 3094, 3095, 3098, 3106, 3107, 3109, 3118, 3122, 3132, 3136, 3139, 3143, 3147, 3156, 3163, 3164, 3170, 3171, 3173, 3177, 3178, 3182, 3184, 3186, 3192, 3196, 3203, 3205, 3208, 3209, 3215, 3217, 3218, 3219, 3220, 3226, 3234, 3236, 3238, 3254, 3258, 3267, 3269, 3272, 3275, 3277, 3278, 3279, 3284, 3293, 3294, 3295, 3301, 3305, 3314, 3315, 3320, 3322, 3333, 3337, 3340, 3341, 3347, 3348, 3351, 3352, 3355, 3356, 3357, 3359, 3360, 3364, 3369, 3371, 3373, 3391, 3393, 3402, 3410, 3413, 3417, 3422, 3424, 3425, 3431, 3435, 3445, 3446, 3447, 3449, 3452, 3454, 3459, 3461, 3468, 3472, 3477, 3480, 3491, 3498, 3502, 3504, 3508, 3519, 3525, 3536, 3541, 3542, 3543, 3549, 3550, 3554, 3557, 3558, 3564, 3583, 3584, 3590, 3604, 3610, 3616, 3620, 3645, 3650, 3651, 3653, 3654, 3656, 3666, 3680, 3681, 3693, 3696, 3701, 3704, 3713, 3732, 3734, 3735, 3747, 3751, 3761, 3767, 3773, 3791, 3792, 3796, 3797, 3802, 3804, 3813, 3821, 3822, 3830, 3831, 3833, 3837, 3838, 3841, 3844, 3856, 3864, 3873, 3880, 3889, 3891, 3897, 3898, 3910, 3914, 3915, 3918, 3919, 3925, 3933, 3937, 3946, 3949, 3951, 3955, 3956, 3971, 3975, 3977, 3979, 3985, 3991, 3993, 4000, 4005, 4010, 4018, 4022, 4025, 4027, 4029, 4037, 4038, 4039, 4041, 4052, 4064, 4065, 4066, 4070, 4077, 4078, 4093, 4095, 4108, 4112, 4118, 4123, 4129, 4136, 4137, 4143, 4149, 4151, 4171, 4175, 4181, 4185, 4188, 4190, 4191, 4198, 4201, 4202, 4207, 4210, 4223, 4233, 4236, 4238, 4240, 4245, 4260, 4272, 4276, 4277, 4278, 4285, 4286, 4292, 4295, 4300, 4301, 4302, 4305, 4312, 4315, 4324, 4325, 4328, 4336, 4346, 4353, 4357, 4358, 4361, 4368, 4378, 4386, 4391, 4395, 4396, 4399, 4400, 4405, 4429, 4448, 4456, 4457, 4466, 4468, 4472, 4478, 4480, 4484, 4485, 4490, 4495, 4504, 4507, 4509, 4511, 4513, 4525, 4538, 4539, 4540, 4546, 4549, 4556, 4557, 4559, 4564, 4565, 4568, 4590, 4593, 4596, 4606, 4609, 4614, 4618, 4621, 4624, 4627, 4631, 4633, 4641, 4643, 4650, 4656, 4657, 4659, 4673, 4674, 4680, 4684, 4689, 4691, 4694, 4703, 4705, 4707, 4713, 4717, 4718, 4721, 4726, 4731, 4732, 4735, 4738, 4741, 4753, 4759, 4761, 4765, 4766, 4767, 4777, 4789, 4791, 4798, 4800, 4801, 4802, 4804, 4811, 4814, 4821, 4822, 4823, 4841, 4842, 4844, 4847, 4851, 4860, 4861, 4862, 4864, 4868, 4872, 4876, 4879, 4890, 4892, 4896, 4898, 4903, 4913, 4922, 4923, 4933, 4937, 4948, 4951, 4952, 4955, 4957, 4958, 4963, 4964, 4972, 4973, 4981, 4984, 4991, 4995, 5006, 5022, 5023, 5033, 5038, 5042, 5046, 5052, 5053, 5063, 5079, 5083, 5085, 5089, 5090, 5099, 5101, 5107, 5111, 5114, 5123, 5125, 5127, 5133, 5134, 5137, 5138, 5146, 5154, 5156, 5175, 5179, 5181, 5185, 5195, 5196, 5197, 5200, 5202, 5204, 5205, 5210, 5219, 5224, 5228, 5230, 5231, 5236, 5243, 5245, 5252, 5253, 5254, 5256, 5258, 5261, 5267, 5272, 5276, 5285, 5289, 5293, 5294, 5305, 5306, 5314, 5319, 5322, 5327, 5336, 5346, 5364, 5366, 5368, 5371, 5378, 5383, 5386, 5387, 5391, 5394, 5396, 5409, 5410, 5411, 5427, 5429, 5433, 5439, 5443, 5445, 5458, 5461, 5462, 5468, 5477, 5483, 5487, 5498, 5499, 5500, 5503, 5509, 5510, 5517]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Normalizing the Audio Data**"
      ],
      "metadata": {
        "id": "qyWmSyGIFYbc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_mean_std(train_loader):\n",
        "    running_sum = 0\n",
        "    running_sum_sq = 0\n",
        "    total_count = 0\n",
        "\n",
        "    for i,batch in enumerate(train_loader):\n",
        "        # if i <= 2:\n",
        "        features_batch, labels_batch, seq_sizes_batch = batch\n",
        "        for features, seq_size in zip(features_batch, seq_sizes_batch):\n",
        "            features = features.numpy()\n",
        "            # print(features[:seq_size].shape)\n",
        "            # print(features[:seq_size].shape,features[:seq_size])\n",
        "            running_sum += np.sum(features[:seq_size], axis=0)\n",
        "            running_sum_sq += np.sum(features[:seq_size] ** 2, axis=0)\n",
        "            total_count += seq_size\n",
        "\n",
        "    mean = running_sum / total_count\n",
        "    std = np.sqrt(running_sum_sq / total_count - mean ** 2)\n",
        "\n",
        "    return mean, std"
      ],
      "metadata": {
        "id": "L8glRn6Ozf5x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mean, std = calculate_mean_std(train_loader_unnormalized)\n",
        "print(mean.shape,mean[:10])\n",
        "print(std.shape,std[:10])"
      ],
      "metadata": {
        "id": "6LrLRjhw1tgv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Results for Acoustic only models:\n",
        "**Train Results:**\n",
        "\n",
        "| Model | Train Loss | Weighted Accuracy | Unweighted Accuracy | Angry Acc. | Happy Acc. | Neutral Acc. | Sad Acc. |\n",
        "|-------|-----------|------------------|---------------------|------------|------------|--------------|----------|\n",
        "| Last Block (epochs=15) | 1.1084 | 49.95% | 51.57% | 52.19% | 37.83% | 48.80% | 67.46% |\n",
        "| Avg. Pool (epochs15) | 0.9579 | 61.41% | 62.69% | 65.69% | 51.56% | 60.72% | 72.79% |\n",
        "| Attention based (attention layer size - 64) (e=14) | 0.6589 | 75.23% | 76.07% | 79.86% | 68.67% | 75.00% | 80.73% | \n",
        "\n",
        "\n",
        "**Test Results:**\n",
        "\n",
        "| Model | Test Loss | Weighted Accuracy | Unweighted Accuracy | Angry Acc. | Happy Acc. | Neutral Acc. | Sad Acc. |\n",
        "|-------|-----------|------------------|---------------------|------------|------------|--------------|----------|\n",
        "| Last Block (e=15) | 1.1296 | 49.82% | 50.03% | 38.79% | 38.44% | 57.91% | 64.97% |\n",
        "| Avg. Pool (e=15) | 1.0815 | 54.98% | 54.45% | 37.38% | 49.38% | 63.00% | 68.02% |\n",
        "| Attention based (attention layer size - 64) (e=14) | 1.0620 | 59.22% | 60.32% | 60.71% | 54.18% | 56.82% | 69.57% | \n",
        "\n",
        "Note: The categorical names used are: Angry, Happy, Neutral, and Sad."
      ],
      "metadata": {
        "id": "HFh3Hkt6rcA3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **lexical Only Model**"
      ],
      "metadata": {
        "id": "LMPeD--Ff4rs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers\n",
        "!pip install contractions"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TIp57nz2x4wd",
        "outputId": "3d783de5-4bba-4a81-8b08-a740c32b3985"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.28.1-py3-none-any.whl (7.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.0/7.0 MB\u001b[0m \u001b[31m48.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.9/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from transformers) (2.27.1)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.9/dist-packages (from transformers) (23.1)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1\n",
            "  Downloading tokenizers-0.13.3-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m68.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting huggingface-hub<1.0,>=0.11.0\n",
            "  Downloading huggingface_hub-0.13.4-py3-none-any.whl (200 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m200.1/200.1 kB\u001b[0m \u001b[31m20.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.9/dist-packages (from transformers) (1.22.4)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.9/dist-packages (from transformers) (2022.10.31)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.9/dist-packages (from transformers) (4.65.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from transformers) (3.11.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.9/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (4.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (2022.12.7)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (1.26.15)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (3.4)\n",
            "Installing collected packages: tokenizers, huggingface-hub, transformers\n",
            "Successfully installed huggingface-hub-0.13.4 tokenizers-0.13.3 transformers-4.28.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BertTokenizer, BertModel\n",
        "import contractions"
      ],
      "metadata": {
        "id": "0JDhNKoAyLSc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "bert_model = BertModel.from_pretrained('bert-base-uncased')\n",
        "tokenizer.add_special_tokens({'additional_special_tokens': ['<sil>', '++laughter++', '++breathing++']}) # Modify the tokenizer to add special tokens\n",
        "bert_model.resize_token_embeddings(len(tokenizer))  # Update the BERT model to account for the new tokens"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 219,
          "referenced_widgets": [
            "e5904489f7e54d8a84b3e4fdf31d1abd",
            "a31a79dc5b044dfbacafccb5ffc17f05",
            "4fc68d44b55541ca9315ea20d794b45a",
            "315f1b0cd77b4ce4bab3171ffaed19f4",
            "1041fe2827f1403287aed166baeee542",
            "0f98de644e77492e8250280a9208abb8",
            "ca0db43570a64f1d9e8878730861c99f",
            "02bc3424850647f2bd46ab2c6c898fa5",
            "172fb4c09eae497783b1d0bf58782741",
            "744a1e9700994fffbef4753312208f9c",
            "077eb24664294a389fb9f215c71c4a9c",
            "4c5297389b2944d7a183ce6cd63ebe14",
            "9182b690df8c419d9e21a516aee53f62",
            "0609cf260ddd496b9185f2332a22836a",
            "ac2109c853244e20a87108e2918acf4b",
            "3defceaba93d4867a71ce15a08320058",
            "56534533a40a4a769995e3d5b783481f",
            "f3a46f3409e14de28646f66c15dde6ee",
            "35df28d6a5dc4cd1a241d71eb10c598b",
            "8e51a9591c4a461882387717e2858bf4",
            "dad2051c10b142308e8d57762cbcf857",
            "3911409a1811418ab0b201d923fd7c90",
            "234609350d0e44d599d84475d7513e81",
            "46ba286a092e478bafc6cd06f857645e",
            "2874883efcb043ecbc17899d4e5a8c2b",
            "558dd1fd24ed49ec81e661ba1c96bd3b",
            "02fadd5eff514810bbc59d2adf847dfb",
            "8486e02695b1405ba19223756efa6bc2",
            "9791324b4b1d4d78b1294074bffa31b1",
            "35d710ecb2604e8c8bd2c3465251e168",
            "b862d5b947904a7db294b1f6f0567219",
            "5750b862aba548669a025357ca64e3b8",
            "fbaeeb2d169c4604bd55e36fd39e02b2",
            "27e4d96e4f594fb6830baaf8292bb4cc",
            "1e93e0d57ae4446989e855adbee5c7b3",
            "9427b0fcf99946f6a689c679406e0503",
            "4ceaf5da512546ec846c8ab131bf2ad4",
            "f16c7cea7f9d48f4b08905c354d6d485",
            "7bd01fbe19f84697912f89d2eb8e6822",
            "c779bd311a94460db91f9b338af0eabf",
            "0985c41ad1f54571bb6bd7fecfa840c2",
            "9b57054dc437439b8050ae28a0b5ab53",
            "1ebc69b7e55e4eb399f46412df238124",
            "f239d8c092f14ceea533db26f5149812"
          ]
        },
        "id": "RZNjNczbySMw",
        "outputId": "0075d94a-0ccf-4527-a4a6-6b85f9decbc3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading (…)solve/main/vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e5904489f7e54d8a84b3e4fdf31d1abd"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading (…)okenizer_config.json:   0%|          | 0.00/28.0 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "4c5297389b2944d7a183ce6cd63ebe14"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading (…)lve/main/config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "234609350d0e44d599d84475d7513e81"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading pytorch_model.bin:   0%|          | 0.00/440M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "27e4d96e4f594fb6830baaf8292bb4cc"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "trans_words = data['trans_words'].copy()"
      ],
      "metadata": {
        "id": "qeF8LHFXuCHn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#  remove the <s>, </s>  tokens from the text before tokenizing \n",
        "def preprocess_text(text,tokenizer):\n",
        "    text = text.lower()\n",
        "    text = contractions.fix(text)  # Expand contractions\n",
        "\n",
        "    # note: since we have lowered the case, we are using garbage instead of GARBAGE \n",
        "    text = text.replace('<s>', '[CLS]').replace('</s>', '[SEP]').replace('++garbage++', '') #.replace('++breathing++', '').replace('++laughter++', '').replace('<sil>', '')\n",
        "    # print(text)\n",
        "    tokens = tokenizer.tokenize(text)\n",
        "    # print(tokens)\n",
        "    input_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
        "    return input_ids\n"
      ],
      "metadata": {
        "id": "J8mREuZ-yWq3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "idx = 18\n",
        "trans_word = trans_words.iloc[idx]\n",
        "lst = trans_word.split()\n",
        "print(len(lst),lst)\n",
        "print('transcribed words:',trans_word)\n",
        "tokenised = preprocess_text(trans_words.iloc[idx],tokenizer)\n",
        "print('transcribed words:',tokenised,len(tokenised))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qbbX_9pnyb3M",
        "outputId": "50dcda41-05e9-4749-d7fc-5d430dd63e78"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "32 ['<s>', '<sil>', 'AW', 'DO', 'YOU', 'KNOW', 'I', 'SHOULD', 'HAVE', 'BROUGHT', 'A', 'SIX', 'PACK', '<sil>', 'TOO', '<sil>', 'A', 'SIX', 'PACK', 'THAT', 'WOULD', 'BE', 'JUST', 'THE', 'TICKET', 'RIGHT', 'ABOUT', 'NOW', '<sil>', 'HUH', '<sil>', '</s>']\n",
            "transcribed words: <s> <sil> AW DO YOU KNOW I SHOULD HAVE BROUGHT A SIX PACK <sil> TOO <sil> A SIX PACK THAT WOULD BE JUST THE TICKET RIGHT ABOUT NOW <sil> HUH <sil> </s>\n",
            "[CLS] <sil> aw do you know i should have brought a six pack <sil> too <sil> a six pack that would be just the ticket right about now <sil> huh <sil> [SEP]\n",
            "['[CLS]', '<sil>', 'aw', 'do', 'you', 'know', 'i', 'should', 'have', 'brought', 'a', 'six', 'pack', '<sil>', 'too', '<sil>', 'a', 'six', 'pack', 'that', 'would', 'be', 'just', 'the', 'ticket', 'right', 'about', 'now', '<sil>', 'huh', '<sil>', '[SEP]']\n",
            "transcribed words: [101, 30522, 22091, 2079, 2017, 2113, 1045, 2323, 2031, 2716, 1037, 2416, 5308, 30522, 2205, 30522, 1037, 2416, 5308, 2008, 2052, 2022, 2074, 1996, 7281, 2157, 2055, 2085, 30522, 9616, 30522, 102] 32\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Reslut for Lexical Only:**\n",
        "**Train Results:**\n",
        "\n",
        "| Model | Train Loss | Weighted Accuracy | Unweighted Accuracy | Angry Acc. | Happy Acc. | Neutral Acc. | Sad Acc. |\n",
        "|-------|-----------|------------------|---------------------|------------|------------|--------------|----------|\n",
        "| Last Block (e=8/10) | 0.6268 | 76.49% | 76.50% | 78.63% | 78.26% | 74.74% | 74.38% |\n",
        "| Avg. Pool (e=9/10) | 0.7926 | 70.70% | 70.29% | 71.65% | 68.73% | 75.71% | 65.08% |\n",
        "| Attention based (attention layer size - 64) (e=7/10) | 0.5780 | 77.65% | 77.60% | 79.53% | 76.74% | 78.86% | 75.28% | \n",
        "\n",
        "Note: The categorical names used are: Angry, Happy, Neutral, and Sad.\n",
        "\n",
        "**Test Results:**\n",
        "\n",
        "| Model | Test Loss | Weighted Accuracy | Unweighted Accuracy | Angry Acc. | Happy Acc. | Neutral Acc. | Sad Acc. |\n",
        "|-------|-----------|------------------|---------------------|------------|------------|--------------|----------|\n",
        "| Last Block (e=8/10) | 0.9218 | 65.04% | 64.03% | 53.74% | 64.06% | 71.31% | 67.01% |\n",
        "| Avg. Pool (e=9/10) | 1.0551 | 62.68% | 60.56% | 52.34% | 70.31% | 67.83% | 51.78% |\n",
        "| Attention based (attention layer size - 64) (e=7/10) | 0.9797 | 63.59% | 61.59% | 54.67% | 70.00% | 68.90% | 52.79% |\n",
        "\n",
        "Note: The categorical names used are: Angry, Happy, Neutral, and Sad."
      ],
      "metadata": {
        "id": "YHdMjx6Otpwi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Why the last block method performed better?\n",
        "    1. Shorter sequences show BLSTM able to capture most of the utterance context in the last block. So the last block has most of the information. But since we have a BLSTM model the other blocks output would also have the context to the entire utterance so averag pooling should also work. But maybe focusing on just one block's output makes it easier for the model to optimize weights sice we have a start and end token that are same across all the samples. It was also evident as when we didnt use the start and end tokens all the three models performed worst and the last block output method didnt do as good.\n",
        "    2. Something to do with BERT\n",
        "    3. Large feature input size so overfitting is possible. The more complex attention model overfits more compared to the simepler non attention based model."
      ],
      "metadata": {
        "id": "qrjW8CDg8pS1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## MM Dataset"
      ],
      "metadata": {
        "id": "yNjapoiPLJ3Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# data_pickel_path = ''"
      ],
      "metadata": {
        "id": "lOMgTLD6aA15"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class IEMOCAP_mm(Dataset):\n",
        "    def __init__(self, tokenizer, bert_model, mean, std, data_pickel_path, max_len=512):\n",
        "        data = pd.read_pickle(data_pickel_path)\n",
        "        self.x_l = data['trans_words']\n",
        "        self.x_a = [np.array(samp_feat) for samp_feat in data['features']]\n",
        "\n",
        "        y = data['emotion'].values\n",
        "        self.label_encoder = LabelEncoder()\n",
        "        self.y = self.label_encoder.fit_transform(y)\n",
        "\n",
        "        self.tokenizer = tokenizer\n",
        "        self.bert_model = bert_model\n",
        "        self.max_len = max_len\n",
        "        self.mean = mean\n",
        "        self.std = std\n",
        "        self.n_samples = data.shape[0]\n",
        "        # self.spk = data['spk'] \n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "\n",
        "        # lexical features\n",
        "        text = self.x_l[idx]\n",
        "        word_ids = preprocess_text(text,self.tokenizer)\n",
        "        word_ids = self.truncate(word_ids)\n",
        "        embeddings = self.extract_embeddings(word_ids) \n",
        "        seq_size_l = len(word_ids)\n",
        "\n",
        "        # acoustic features\n",
        "        feat_a = torch.tensor(self.x_a[idx])\n",
        "        if self.mean is not None and self.std is not None:\n",
        "            feat_a = (feat_a - self.mean) / self.std\n",
        "        seq_size_a = feat_a.shape[0]\n",
        "\n",
        "        return feat_a,seq_size_a,torch.tensor(embeddings),seq_size_l,torch.tensor(self.y[idx])\n",
        "\n",
        "    def truncate(self, sequence):\n",
        "        if len(sequence) > self.max_len:\n",
        "            return sequence[:self.max_len]\n",
        "        else:\n",
        "            return sequence # + [0] * (self.max_len - len(sequence))\n",
        "\n",
        "    def create_attention_mask(self, input_ids):\n",
        "        return [1 if token_id > 0 else 0 for token_id in input_ids]\n",
        "\n",
        "    def extract_embeddings(self, input_ids):#, attention_mask):\n",
        "        with torch.no_grad():\n",
        "            input_ids = torch.tensor(input_ids).unsqueeze(0)\n",
        "            # attention_mask = torch.tensor(attention_mask).unsqueeze(0)\n",
        "            outputs = self.bert_model(input_ids)#, attention_mask=attention_mask)\n",
        "            embeddings = outputs.last_hidden_state.squeeze(0).numpy()\n",
        "        return embeddings\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.n_samples\n",
        "\n",
        "    def get_spk(self):\n",
        "        return self.spk\n",
        "        \n",
        "    def get_encoder(self):\n",
        "        return self.label_encoder"
      ],
      "metadata": {
        "id": "pPz7Oe4QLRe8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Collate function for MM**"
      ],
      "metadata": {
        "id": "DeF_ALmx1vvj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def custom_collate_fn_mm(batch):\n",
        "    feat_a, seq_size_a, feat_l, seq_size_l, labels = zip(*batch)\n",
        "\n",
        "    # Pad the sequences\n",
        "    padded_feat_a = pad_sequence(feat_a, batch_first=True)\n",
        "    padded_feat_l = pad_sequence(feat_l, batch_first=True)\n",
        "\n",
        "    # Convert sequence lengths and labels to tensors\n",
        "    seq_size_a = torch.tensor(seq_size_a)\n",
        "    seq_size_l = torch.tensor(seq_size_l)\n",
        "    labels = torch.tensor(labels)\n",
        "\n",
        "    return padded_feat_a, seq_size_a, padded_feat_l, seq_size_l, labels\n"
      ],
      "metadata": {
        "id": "nPtYGyrl-3S1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_mm = IEMOCAP_mm(tokenizer, bert_model,mean,std,data_pickel_path)\n",
        "feat = dataset_mm[0]\n",
        "print(feat[0].shape,feat[2].shape,feat)"
      ],
      "metadata": {
        "id": "ioHSr6CWZq37"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_samp = len(dataset_mm)\n",
        "train_size = int(0.8 * len(dataset_mm))\n",
        "test_size = len(dataset_mm) - train_size\n",
        "train_indices = random.sample(range(num_samp), train_size)\n",
        "test_indices = [i for i in range(num_samp) if i not in train_indices ]\n",
        "print(train_indices)\n",
        "print(test_indices)"
      ],
      "metadata": {
        "id": "naqKfZPk101H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 32\n",
        "\n",
        "train_dataset_mm = torch.utils.data.Subset(dataset_mm, train_indices)\n",
        "train_loader_mm = DataLoader(dataset=train_dataset_mm, batch_size=batch_size, shuffle=True,collate_fn=custom_collate_fn_mm)\n",
        "\n",
        "test_dataset_mm = torch.utils.data.Subset(dataset_mm, test_indices)\n",
        "test_loader_mm = DataLoader(dataset=test_dataset_mm, batch_size=batch_size, shuffle=False,collate_fn=custom_collate_fn_mm)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "90a196ec-570d-46be-e725-c6c6c5aeb628",
        "id": "gikBGBLnb_OC"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[5238, 912, 204, 2253, 2006, 1828, 1143, 839, 4467, 712, 4837, 3456, 260, 244, 767, 1791, 1905, 4139, 4931, 217, 4597, 1628, 5323, 4464, 3436, 1805, 3679, 4827, 2278, 53, 1307, 3462, 2787, 2276, 1273, 1763, 2757, 837, 759, 3112, 792, 2940, 2817, 4945, 2166, 355, 3763, 4392, 1022, 3100, 645, 4522, 2401, 5149, 5066, 2962, 4729, 1575, 569, 375, 5417, 1866, 2370, 653, 1907, 827, 3113, 2277, 3714, 5207, 2988, 1332, 3032, 2910, 1716, 2187, 5308, 584, 4990, 5201, 1401, 4375, 2005, 1338, 3786, 3108, 2211, 5242, 4562, 1799, 2656, 458, 1876, 262, 2584, 3286, 2193, 542, 1728, 4646, 2577, 1741, 5369, 4089, 3241, 5266, 3758, 1170, 2169, 5513, 2020, 4598, 4415, 2152, 4788, 3509, 4780, 3271, 2965, 1796, 1133, 4174, 4042, 744, 385, 898, 1252, 5140, 1310, 3458, 4885, 520, 3152, 3126, 4881, 3834, 4334, 2059, 4532, 94, 938, 4398, 2185, 5250, 2786, 913, 2404, 3561, 1295, 3716, 26, 2157, 4100, 1463, 4158, 871, 5122, 2444, 5234, 5365, 4988, 1629, 5393, 3063, 1323, 4418, 4344, 4, 4906, 2655, 4002, 159, 916, 2973, 2519, 1961, 474, 1973, 4647, 5469, 701, 3981, 566, 4363, 1030, 1051, 3893, 4503, 1352, 2171, 4322, 4969, 3466, 1735, 4417, 1647, 2553, 3268, 3059, 3588, 4239, 3698, 991, 2030, 1840, 524, 2769, 172, 4819, 4537, 1885, 4820, 1804, 58, 581, 5169, 482, 1875, 552, 257, 2706, 580, 4211, 1949, 2281, 3976, 1755, 5325, 1083, 4677, 4720, 3872, 1990, 3874, 3334, 1559, 772, 794, 3531, 2902, 3469, 3367, 3825, 443, 806, 496, 3298, 2779, 895, 2036, 1569, 1558, 4393, 3675, 1148, 5508, 1503, 5295, 3789, 2046, 617, 3630, 4508, 802, 414, 4428, 120, 764, 1936, 1362, 3329, 3978, 3943, 1751, 3285, 480, 1348, 3104, 17, 3198, 2172, 3727, 2336, 3465, 4552, 3986, 1268, 1555, 2430, 1783, 479, 4744, 4441, 499, 2569, 468, 410, 4785, 3905, 4119, 4350, 1289, 465, 4160, 656, 1522, 561, 4874, 556, 1926, 3307, 982, 4666, 2016, 4742, 4870, 325, 5073, 671, 3434, 4781, 4630, 4282, 2591, 2136, 1673, 2573, 1955, 2175, 3242, 1072, 2457, 3745, 2590, 594, 76, 3754, 5088, 4612, 819, 600, 4404, 1746, 4144, 5239, 1085, 2859, 563, 2001, 3027, 2334, 1292, 3589, 4450, 2478, 5010, 4333, 64, 4543, 2452, 848, 1100, 5475, 945, 876, 5381, 5485, 2231, 2308, 4954, 1725, 2808, 1667, 2162, 4140, 5349, 2057, 416, 756, 5279, 2266, 361, 29, 2732, 1071, 2145, 5355, 3619, 4519, 3503, 4594, 79, 5347, 616, 1221, 4469, 295, 3024, 4771, 4526, 1213, 3520, 1044, 342, 2525, 2987, 326, 2931, 1720, 2044, 842, 2897, 4586, 5249, 5084, 1266, 1939, 1331, 1450, 3377, 203, 1469, 2721, 3372, 2032, 5377, 1304, 885, 3133, 317, 3855, 1822, 1634, 3770, 2864, 2500, 1864, 1826, 193, 1582, 3264, 2689, 2282, 568, 2286, 2876, 4173, 3274, 5472, 2712, 226, 944, 2139, 1462, 4756, 2174, 313, 888, 4887, 3559, 2831, 5225, 3574, 4966, 4189, 947, 3155, 4723, 1557, 2086, 363, 3572, 13, 4259, 4410, 1614, 2983, 3533, 573, 2704, 2571, 1020, 2460, 4154, 2533, 3345, 2672, 3296, 2422, 4541, 1042, 1571, 3444, 3105, 1425, 4662, 2465, 3326, 4488, 3, 2489, 2350, 1721, 3521, 4751, 5328, 2639, 3809, 5132, 3622, 1750, 4187, 3876, 1390, 694, 2324, 4222, 2745, 765, 1924, 2542, 5315, 1631, 1207, 200, 378, 5437, 3892, 596, 3730, 3395, 4715, 1592, 3145, 4049, 3273, 1998, 1208, 45, 873, 3482, 1792, 1440, 4243, 3805, 411, 4566, 2041, 994, 3739, 1092, 3806, 4351, 4578, 4877, 2599, 3625, 4135, 3495, 5021, 3652, 1303, 3888, 3686, 2123, 2025, 2271, 4270, 3969, 1959, 2249, 3603, 634, 2340, 1920, 2225, 2751, 2619, 4424, 660, 5399, 1235, 1894, 3137, 1251, 1752, 526, 3398, 3339, 2710, 4445, 3816, 3406, 510, 1694, 3441, 3190, 4784, 160, 4716, 3116, 3907, 48, 2881, 2446, 3194, 3432, 4409, 4473, 1806, 3999, 1797, 2235, 3570, 5248, 237, 3185, 2753, 3312, 5331, 3828, 1045, 5438, 220, 3227, 4848, 4623, 222, 687, 3511, 1111, 3782, 1488, 4974, 2131, 5026, 2681, 1733, 3724, 2677, 2764, 4884, 2279, 3453, 2066, 670, 3852, 158, 5354, 426, 2866, 1836, 562, 329, 254, 5150, 1633, 166, 1248, 1954, 1034, 3879, 937, 4620, 1785, 5012, 2099, 3021, 1374, 5379, 1341, 2548, 5092, 4740, 210, 2555, 4920, 3074, 3249, 1624, 622, 1989, 834, 2470, 5317, 4636, 336, 2844, 4364, 5404, 3035, 564, 5176, 2795, 103, 4924, 4015, 864, 3551, 2967, 3766, 1253, 3567, 1442, 4274, 2212, 4408, 3960, 3808, 3568, 2198, 2640, 2011, 709, 2284, 3692, 1997, 4968, 4668, 5432, 2755, 235, 4985, 2662, 1489, 3994, 1737, 2906, 2116, 2788, 2290, 2263, 4553, 83, 4232, 1565, 5339, 1977, 5106, 5144, 4548, 1968, 3900, 4020, 3671, 141, 762, 2410, 1815, 4901, 1993, 2508, 4764, 3023, 5007, 4534, 4349, 2816, 3485, 5257, 2709, 2882, 3717, 2219, 2511, 5382, 1888, 988, 1577, 5425, 979, 4389, 1516, 5269, 1772, 3966, 2265, 4297, 2318, 823, 1590, 2426, 1863, 2956, 5098, 2476, 115, 4897, 1036, 2247, 372, 446, 4533, 2393, 4859, 4021, 840, 100, 4702, 2329, 3845, 3921, 3608, 2791, 1510, 420, 2068, 3913, 934, 535, 3282, 4028, 606, 439, 1242, 1222, 4610, 5019, 697, 2033, 970, 4571, 3409, 1848, 4280, 4919, 3690, 3626, 2435, 3512, 2501, 4658, 493, 812, 1702, 5421, 2167, 665, 1286, 1964, 1423, 4521, 614, 1282, 21, 3346, 4693, 3849, 2385, 267, 1896, 2360, 2316, 3719, 583, 1912, 4683, 1620, 4979, 940, 4461, 1841, 1220, 2176, 1165, 5442, 488, 1359, 5345, 2364, 3597, 1018, 3839, 2491, 3297, 2230, 4099, 4423, 4045, 3586, 658, 5113, 3539, 4808, 2051, 211, 748, 5302, 169, 2207, 4866, 1435, 3854, 4251, 5164, 5491, 1486, 5351, 4708, 747, 3850, 2850, 5034, 2730, 2630, 856, 1317, 2701, 5096, 4058, 2361, 3280, 4506, 300, 3725, 721, 2576, 2067, 2648, 949, 3311, 4215, 9, 4444, 3784, 3385, 444, 1536, 4247, 2963, 4083, 3621, 422, 5147, 5444, 4499, 1073, 2359, 5167, 3970, 4971, 236, 1960, 1297, 2545, 4512, 112, 4524, 3342, 763, 4998, 929, 3780, 962, 1261, 4082, 2390, 4168, 2239, 3403, 3952, 3868, 1996, 3741, 4515, 1184, 3142, 1561, 4163, 1118, 571, 5405, 3399, 2784, 4159, 2188, 4675, 2317, 2445, 4011, 1217, 3658, 4412, 3967, 2827, 2723, 4678, 4451, 3090, 5441, 2636, 1545, 1956, 4936, 1913, 3365, 357, 2606, 5286, 3123, 3162, 1246, 4057, 303, 4725, 4114, 2719, 822, 3606, 816, 4308, 3743, 125, 1180, 3358, 1264, 612, 3846, 5330, 2773, 3256, 657, 2691, 4371, 5453, 2594, 3997, 4432, 294, 560, 1923, 2354, 4737, 740, 3555, 5275, 4846, 3634, 5376, 2453, 4904, 376, 2657, 459, 2403, 2936, 3070, 3528, 1192, 2000, 4967, 3375, 1475, 1392, 1434, 646, 5091, 1972, 4076, 1172, 1902, 3777, 2080, 3764, 2091, 5184, 3811, 2356, 1294, 605, 3618, 2830, 2450, 3475, 2048, 3742, 2474, 4997, 3151, 3958, 4980, 1943, 3124, 5478, 2423, 2418, 179, 5190, 2248, 66, 401, 4069, 2344, 5309, 2886, 1794, 5051, 2053, 1120, 795, 322, 2530, 3611, 273, 5449, 1076, 738, 2417, 2676, 4560, 1438, 1644, 1082, 2997, 4348, 4110, 2232, 1347, 2105, 3947, 5310, 2774, 943, 3836, 5126, 1153, 4696, 3255, 2996, 739, 3232, 114, 5157, 1012, 4605, 3019, 2147, 3121, 3043, 887, 1915, 3862, 205, 2687, 1813, 517, 3803, 2475, 3344, 955, 1145, 371, 304, 2493, 4035, 951, 796, 4489, 4889, 3183, 5370, 3039, 3433, 1265, 4932, 811, 4008, 3343, 2291, 268, 4829, 1779, 3632, 3642, 1934, 2971, 813, 3009, 2938, 5274, 3261, 2260, 1554, 1000, 4385, 750, 4793, 174, 5255, 5136, 1995, 1031, 1681, 4867, 1697, 1769, 1908, 4497, 4982, 23, 4953, 1185, 1064, 4422, 1429, 900, 4634, 1079, 121, 2934, 5296, 2652, 129, 1427, 2173, 429, 1038, 3448, 931, 5388, 3901, 3672, 5401, 4204, 893, 3702, 4127, 1814, 5474, 4271, 4836, 3752, 255, 498, 3923, 3290, 3492, 884, 4016, 3633, 602, 661, 2638, 1215, 538, 1033, 2252, 2663, 3120, 2415, 4359, 4141, 3524, 4686, 4857, 1761, 3523, 3699, 1871, 3389, 2776, 3715, 3266, 3407, 778, 2560, 3496, 4254, 2088, 3066, 1250, 3885, 549, 4856, 699, 4570, 3537, 791, 3052, 1065, 491, 2700, 1001, 4572, 2896, 3464, 421, 4452, 2559, 2880, 5159, 4156, 1742, 1267, 3950, 1837, 886, 2868, 3011, 941, 5077, 1852, 3515, 215, 2190, 4479, 1477, 2238, 2531, 2783, 2875, 50, 4893, 1173, 3283, 570, 1162, 251, 751, 1762, 3081, 3439, 4269, 2792, 5218, 3031, 2552, 4477, 695, 430, 1274, 4195, 407, 668, 2229, 3629, 3473, 4905, 4588, 3392, 2237, 1765, 932, 4535, 5016, 908, 2320, 5361, 2526, 5463, 4910, 3237, 448, 62, 1674, 2469, 1730, 1124, 2093, 2371, 4376, 5208, 63, 4074, 3527, 1439, 1058, 3114, 4426, 4098, 2901, 590, 3252, 346, 3573, 153, 5311, 637, 2564, 3516, 3313, 3421, 5216, 4397, 3318, 170, 2660, 1407, 3769, 2964, 4604, 3577, 867, 4769, 3569, 4608, 644, 4492, 2541, 2781, 5121, 2728, 1377, 625, 4711, 1588, 2862, 5074, 1209, 1935, 5109, 1199, 2096, 1616, 1421, 5357, 5259, 1451, 4047, 3800, 3677, 2647, 2589, 1236, 3602, 279, 5476, 1811, 2586, 1240, 4339, 1125, 5031, 230, 1441, 2078, 4516, 1271, 1891, 1851, 154, 233, 5359, 4927, 1175, 314, 4834, 4637, 4003, 5130, 369, 4651, 2433, 2076, 1574, 1895, 2377, 2270, 3908, 3243, 3026, 3669, 168, 1842, 3723, 3317, 2341, 2669, 771, 1316, 5165, 1948, 4316, 4103, 3922, 253, 1845, 423, 3321, 3682, 3429, 1406, 4023, 2925, 345, 4875, 2646, 706, 4921, 1014, 2898, 1793, 5430, 2146, 2141, 2497, 650, 1490, 1527, 3759, 1158, 1586, 4165, 3172, 1385, 2780, 2448, 214, 4748, 4750, 2651, 1370, 269, 1350, 387, 2285, 2778, 1583, 1163, 1032, 4130, 3488, 5078, 4225, 3712, 2468, 3576, 615, 1365, 333, 2386, 2718, 579, 5183, 1432, 1270, 3963, 4545, 2860, 5070, 1605, 528, 2437, 2903, 3842, 347, 5233, 2289, 1542, 2635, 4122, 1345, 3330, 523, 2744, 2878, 4989, 4858, 4059, 2807, 3737, 2156, 382, 4033, 2746, 1734, 2082, 1482, 74, 1485, 4356, 4413, 3895, 877, 1399, 3881, 3138, 1991, 786, 927, 5172, 4947, 316, 1211, 5390, 3228, 4331, 2079, 3157, 2210, 3420, 3025, 5011, 4440, 4079, 2711, 1379, 4581, 5247, 5297, 536, 4915, 1543, 631, 664, 4486, 3405, 2837, 3158, 2558, 5502, 3697, 678, 2953, 4978, 178, 1682, 1492, 2770, 2947, 972, 1819, 5105, 1167, 4199, 4250, 3210, 1838, 958, 4585, 4749, 4226, 4770, 3212, 1921, 4294, 3929, 5282, 1506, 4071, 3962, 5237, 5307, 4081, 5227, 1154, 3187, 1564, 4754, 2160, 1714, 663, 4617, 817, 3281, 4734, 5337, 3575, 1024, 213, 2626, 4778, 3580, 1520, 4319, 3826, 4714, 2913, 4234, 2113, 3486, 4926, 3896, 343, 3125, 5215, 1117, 4569, 3708, 2102, 603, 5045, 5504, 5460, 3729, 909, 3867, 1847, 3623, 1431, 4453, 109, 1699, 218, 1623, 2056, 1531, 965, 1581, 334, 1535, 919, 4733, 1305, 3812, 405, 3437, 2927, 4796, 1373, 5415, 598, 3939, 156, 4075, 3757, 4355, 5178, 2851, 3404, 5076, 5434, 2889, 4248, 4224, 2520, 5161, 3710, 324, 77, 1048, 883, 3883, 5332, 4730, 2978, 4366, 2161, 4882, 455, 3179, 4607, 973, 1233, 5226, 195, 976, 1719, 2617, 3251, 2551, 1872, 5300, 454, 3427, 3707, 2047, 5362, 2195, 67, 2588, 2110, 2355, 990, 2943, 588, 1193, 1758, 6, 2518, 1445, 985, 2337, 1706, 5505, 5097, 2737, 350, 2144, 4213, 277, 2154, 2228, 4084, 3221, 3932, 2269, 4786, 1599, 4128, 1925, 3904, 2603, 1584, 1529, 4303, 3061, 4430, 5465, 3229, 276, 1412, 987, 3002, 5329, 5193, 424, 3160, 2383, 4221, 3099, 1361, 546, 181, 1443, 2236, 1386, 3332, 2632, 717, 3401, 3191, 2804, 1903, 2848, 4577, 2587, 746, 3323, 5301, 258, 2932, 3779, 1874, 151, 1201, 825, 4433, 3240, 3866, 3628, 171, 5168, 3984, 2111, 4442, 5501, 2224, 5103, 1037, 149, 3085, 2649, 782, 1171, 3721, 5281, 5240, 3774, 3546, 2685, 4652, 1119, 509, 5072, 1507, 1789, 3643, 1638, 3044, 1801, 5403, 1388, 5001, 5095, 2834, 2037, 1504, 3265, 2126, 5014, 3865, 338, 4595, 1738, 323, 5484, 2467, 3368, 4390, 2234, 1203, 1315, 3853, 327, 1343, 2707, 1210, 1255, 5082, 4794, 2939, 1745, 682, 2825, 1818, 5036, 1831, 173, 2977, 3563, 1444, 3750, 1781, 4162, 4942, 4854, 234, 307, 2749, 2611, 1663, 5061, 2101, 3638, 3071, 2782, 655, 127, 4654, 3476, 2488, 2777, 3200, 4941, 142, 4374, 275, 966, 3188, 2642, 1500, 1483, 1568, 2323, 132, 4797, 628, 4120, 4056, 1519, 1523, 3614, 4719, 4228, 5018, 3807, 2168, 5263, 1630, 1287, 4133, 1141, 5471, 464, 106, 3013, 4773, 5110, 2120, 1585, 2300, 5303, 5189, 3174, 1066, 2883, 1827, 878, 2506, 1169, 2842, 4807, 5180, 502, 555, 3493, 4589, 4080, 707, 2921, 1823, 359, 3319, 2793, 3463, 1309, 2735, 4679, 4792, 265, 4273, 2220, 1189, 1228, 3487, 5340, 2915, 5118, 5003, 2612, 713, 3250, 1480, 2083, 918, 497, 4483, 3244, 3799, 969, 5431, 2023, 107, 1478, 3733, 4427, 1511, 1914, 3291, 2259, 531, 3540, 353, 4007, 5104, 3560, 4474, 2944, 3694, 2153, 3903, 3150, 1677, 4611, 4960, 302, 513, 1298, 2631, 3532, 1843, 4323, 2785, 2119, 1411, 525, 3983, 3195, 2258, 2620, 2407, 745, 3144, 4182, 4745, 3871, 5069, 4712, 508, 2122, 626, 1245, 674, 4558, 1321, 2905, 923, 1417, 2125, 4001, 4482, 1026, 804, 2601, 2256, 3605, 512, 4252, 4090, 2516, 5094, 3957, 2058, 2625, 690, 2424, 2378, 3916, 4783, 2698, 2557, 2951, 4145, 1382, 5151, 4517, 3380, 116, 332, 186, 2627, 3911, 2362, 1084, 2667, 863, 3141, 2343, 3746, 3920, 3771, 124, 2039, 2568, 4184, 1186, 2629, 1237, 1978, 1003, 5032, 3306, 2805, 3598, 1218, 1857, 298, 2820, 245, 647, 1800, 4685, 1983, 4459, 835, 1393, 2484, 4172, 1280, 2942, 5489, 3007, 1414, 4863, 4710, 3115, 1517, 2109, 3548, 434, 3631, 3756, 1910, 501, 1095, 4688, 1015, 577, 396, 207, 1188, 1573, 3794, 1712, 1016, 654, 3884, 4849, 2366, 3906, 3370, 777, 3566, 652, 2040, 2108, 4664, 2042, 1263, 4959, 5380, 368, 1610, 2070, 3768, 986, 881, 2389, 3749, 199, 3349, 1152, 2027, 2447, 2679, 2754, 1927, 5412, 4809, 33, 440, 4178, 548, 5291, 2979, 1498, 3128, 1651, 3303, 185, 1640, 209, 2335, 2301, 797, 1484, 5139, 1182, 301, 5080, 2065, 1844, 3130, 3518, 4369, 4193, 5375, 2502, 486, 527, 3350, 1613, 1528, 5071, 3647, 4805, 1497, 3092, 4148, 815, 4347, 2085, 4813, 4929, 164, 3299, 5348, 5213, 2922, 1364, 1940, 2127, 4261, 610, 2483, 5414, 572, 3624, 4755, 5093, 4682, 1609, 5115, 3028, 1225, 2431, 4117, 2077, 2087, 2180, 5515, 5280, 2305, 1227, 1945, 68, 1508, 1356, 2759, 4167, 3400, 4563, 1260, 2970, 2818, 5419, 4899, 3762, 4438, 1087, 3659, 3206, 3687, 2369, 2365, 4177, 2954, 4845, 4895, 1967, 698, 4383, 4603, 3673, 2538, 4281, 4642, 5166, 4790, 992, 4307, 2345, 2869, 450, 781, 3790, 4169, 1284, 1715, 620, 1691, 2828, 3374, 1680, 2055, 3545, 2504, 1931, 3553, 3304, 4287, 2890, 565, 2124, 849, 2297, 5102, 3980, 4950, 5406, 3917, 1285, 4806, 1882, 2183, 1402, 4179, 4102, 2768, 2952, 4640, 2634, 2839, 4407, 2498, 3376, 787, 1009, 1142, 4004, 4704, 921, 1219, 1183, 2887, 4360, 4176, 5264, 2002, 3471, 1965, 1428, 2295, 2955, 4421, 1178, 3775, 2348, 2771, 4298, 5232, 4106, 1413, 599, 5145, 5312, 4463, 4036, 3438, 1418, 4092, 2686, 1050, 4055, 1962, 5155, 5454, 2206, 1110, 2302, 3801, 1112, 4326, 447, 2521, 3034, 4475, 980, 993, 4962, 2743, 5191, 924, 2613, 953, 4216, 5223, 1692, 4900, 4341, 4345, 4886, 2790, 3609, 3793, 2254, 5496, 1666, 2674, 4048, 5192, 2664, 4510, 1176, 3627, 4072, 2645, 242, 366, 2671, 2351, 4831, 2191, 3928, 2961, 152, 3700, 1711, 720, 148, 1626, 2029, 4244, 4318, 5492, 37, 3148, 2326, 3927, 4706, 1372, 4655, 1861, 3859, 4944, 3207, 3497, 4591, 3288, 1917, 3843, 789, 461, 1354, 3248, 2994, 1879, 5350, 1053, 3755, 3552, 57, 3017, 1380, 4321, 2325, 2762, 4242, 718, 3595, 2616, 1660, 5246, 5413, 5086, 4291, 1641, 2741, 390, 5141, 575, 3078, 1325, 4496, 27, 1068, 5028, 964, 1829, 1093, 4006, 4096, 4671, 2960, 2346, 46, 5135, 4880, 1472, 3912, 967, 4865, 3522, 484, 1906, 1256, 4352, 3961, 2810, 3426, 4916, 5153, 4787, 5244, 3018, 5363, 1505, 2517, 904, 897, 547, 1958, 627, 4487, 2481, 1530, 1703, 4209, 4949, 1928, 2201, 2720, 894, 1013, 4709, 2440, 335, 3169, 3000, 2163, 4758, 1481, 319, 2310, 4476, 252, 2243, 3325, 828, 3197, 3310, 613, 673, 3014, 5389, 1809, 476, 3886, 841, 2388, 3140, 4170, 2090, 1824, 227, 3383, 540, 3840, 1701, 4584, 2309, 5056, 2288, 1893, 4799, 1262, 89, 1621, 5029, 12, 892, 2368, 299, 3263, 5326, 3851, 3510, 261, 2216, 246, 282, 1933, 3662, 4975, 4017, 737, 554, 3408, 3072, 5017, 1533, 1566, 3795, 1546, 1538, 328, 2796, 4930, 2208, 544, 4411, 1424, 485, 731, 2200, 3247, 4387, 521, 5518, 14, 93, 4241, 4669, 2758, 4644, 5163, 2178, 1553, 4660, 3135, 1886, 4824, 635, 1129, 5283, 462, 131, 5040, 1717, 3607, 5162, 3748, 844, 3582, 413, 2432, 137, 5514, 4013, 2752, 3526, 1002, 3706, 5324, 4908, 959, 3930, 889, 231, 5041, 2063, 4554, 2833, 4582, 1306, 3214, 2449, 3909, 1313, 4699, 4406, 586, 3798, 2135, 905, 1693, 1230, 1127, 249, 2280, 4435, 3062, 4268, 4014, 1749, 5486, 5316, 192, 1410, 4580, 3600, 5235, 2148, 4928, 1705, 1672, 611, 4217, 1541, 753, 2203, 4775, 4757, 3442, 1231, 591, 4869, 2917, 2303, 1690, 3934, 4219, 2170, 3041, 4739, 4579, 5117, 832, 4531, 4601, 4290, 4622, 4125, 4994, 5423, 780, 4313, 1099, 1544, 3230, 2477, 4164, 161, 3382, 770, 2406, 2750, 3972, 5062, 1963, 855, 1363, 1241, 4166, 4293, 4255, 2975, 5344, 5470, 4372, 5482, 4840, 3101, 2261, 4613, 1159, 5284, 1909, 3428, 4192, 1471, 4536, 2992, 1473, 2100, 5270, 3467, 403, 4362, 1311, 5424, 1518, 1281, 1695, 187, 2304, 906, 597, 3722, 3003, 3292, 2392, 2372, 1710, 4111, 624, 803, 1351, 942, 3302, 2333, 1006, 2045, 2255, 4883, 3415, 1054, 5171, 2628, 2009, 1887, 689, 4816, 693, 2829, 4212, 4470, 2789, 3517, 2690, 240, 4026, 138, 4367, 198, 5459, 1689, 3668, 2583, 5054, 278, 618, 3053, 5271, 2071, 1867, 3276, 247, 4871, 4830, 3246, 1979, 4983, 3695, 3004, 2179, 3489, 1684, 4917, 3233, 3562, 4088, 2194, 1174, 70, 5143, 4695, 734, 438, 394, 4381, 609, 5112, 3941, 3204, 3926, 4815, 3161, 1094, 1625, 3995, 3829, 5129, 1880, 2314, 996, 925, 4843, 330, 4235, 3765, 2981, 2861, 2760, 4779, 4309, 229, 2608, 38, 3787, 3637, 5473, 1777, 3231, 4542, 3010, 2949, 4332, 4220, 865, 2321, 1947, 3110, 190, 2675, 5265, 3129, 3414, 134, 2641, 1288, 4194, 4229, 5373, 3270, 3674, 315, 3501, 3103, 2918, 984, 2598, 1593, 2214, 1358, 1572, 2684, 321, 4635, 1416, 219, 400, 1790, 950, 311, 1395, 3416, 2514, 2439, 4760, 1336, 4311, 3657, 4043, 4493, 4115, 1457, 4743, 1537, 1768, 760, 2399, 1556, 351, 5035, 4762, 3875, 4284, 4833, 1097, 4138, 2546, 3931, 1595, 4576, 3740, 436, 374, 3084, 4873, 1821, 1474, 1104, 418, 5422, 4523, 5187, 5209, 4992, 5262, 2272, 2098, 1671, 5397, 4253, 367, 1449, 4062, 381, 5030, 2822, 1328, 5015, 4208, 1195, 4925, 4514, 4146, 5124, 4500, 2342, 2134, 5005, 4330, 2496, 3500, 1101, 3776, 2761, 2019, 3924, 259, 684, 1780, 1130, 1724, 3235, 3153, 2582, 1479, 5335, 1010, 2802, 2843, 800, 4632, 4073, 2892, 1247, 2654, 5024, 1360, 2537, 1349, 1803, 1371, 3579, 2434, 557, 2812, 3381, 4574, 4152, 4681, 727, 2670, 4505, 3753, 4976, 2358, 3199, 1334, 1540, 2593, 2873, 1495, 466, 2579, 818, 2425, 3394, 736, 5516, 110, 1890, 858, 1795, 2451, 283, 1675, 2696, 4296, 3430, 4835, 4555, 2536, 3781, 2615, 4183, 1839, 2456, 2384, 3012, 4544, 2075, 1713, 2226, 150, 2990, 2199, 348, 4878, 3005, 1454, 681, 2836, 2306, 1636, 2574, 2462, 433, 43, 3744, 1059, 5384, 4455, 3175, 3149, 4063, 5222, 2814, 1825, 1532, 1602, 3593, 2049, 4046, 3718, 4722, 4782, 428, 4625, 5464, 4575, 475, 5298, 1426, 1277, 1405, 1865, 847, 4024, 3390, 5299, 1950, 399, 3585, 1853, 2824, 2845, 1229, 183, 471, 92, 5457, 3935, 4661, 999, 4619, 714, 4338, 3592, 1355, 2293, 5177, 1892, 4600, 1659, 755, 3056, 1770, 3050, 119, 808, 3353, 1759, 4987, 18, 879, 4109, 1140, 256, 5142, 2409, 3646, 3300, 1335, 807, 4956, 2442, 1074, 2004, 4888, 1283, 1591, 1607, 467, 5025, 4085, 2473, 4032, 3384, 3974, 3387, 4676, 3547, 977, 4249, 5229, 749, 4335, 3987, 2092, 2624, 2413, 4828, 4197, 1732, 4894, 4126, 2958, 3166, 2849, 683, 4993, 4986, 574, 766, 4776, 3832, 5488, 1470, 39, 1984, 559, 799, 5451, 2941, 3316, 2429, 2014, 5100, 4817, 4258, 685, 3599, 126, 903, 1196, 133, 4150, 920, 2202, 3490, 688, 1873, 2028, 2251, 2296, 463, 2312, 2539, 2592, 1502, 2680, 4134, 1807, 5131, 2665, 5418, 419, 5173, 3146, 5057, 2767, 4414, 4306, 356, 1798, 2742, 2819, 859, 1436, 1753, 679, 2427, 846, 2907, 243, 2412, 3239, 1151, 3111, 3982, 3006, 1198, 3055, 3354, 2107, 5519, 3534, 1685, 3594, 1135, 519, 543, 3556, 3336, 1319, 2241, 1608, 2604, 582, 5158, 1696, 4592, 3663, 309, 2510, 1650, 3253, 1139, 4283, 1862, 1999, 1340, 10, 5408, 1877, 1467, 2487, 5358, 2908, 3419, 3965, 2026, 2490, 1322, 1161, 1383, 534, 1987, 4132, 3080, 188, 5008, 3165, 3636, 3450, 3379, 2242, 1869, 2, 4934, 4436, 3260, 3224, 651, 1122, 2618, 1476, 1025, 337, 4810, 677, 212, 5277, 4935, 2826, 4288, 373, 389, 5352, 874, 1953, 2946, 4256, 926, 3257, 5182, 2610, 3309, 2872, 4687, 1729, 541, 4527, 290, 4265, 4746, 365, 2683, 4587, 2315, 2972, 2581, 5416, 1567, 545, 585, 280, 34, 672, 1802, 4912, 872, 3189, 1686, 1812, 5059, 3180, 3087, 2572, 504, 5075, 3613, 5067, 2809, 4147, 4030, 1134, 4365, 4902, 3847, 3222, 4938, 2930, 2854, 1604, 5333, 223, 2736, 3507, 1580, 3036, 4529, 5260, 2121, 49, 2479, 1296, 3479, 3726, 1723, 1496, 3530, 1929, 2914, 3667, 5450, 56, 4583, 3788, 163, 2661, 3483, 4206, 2920, 3954, 2835, 2991, 4116, 3783, 3678, 3938, 2756, 1047, 2858, 3051, 4329, 1654, 318, 1098, 669, 4443, 3684, 1299, 1563, 1709, 1330, 1394, 2734, 135, 2856, 1430, 2240, 1494, 1952, 4087, 4653, 118, 500, 176, 4690, 4853, 4214, 928, 4818, 4518, 4573, 633, 3259, 2717, 4402, 4747, 4155, 5506, 25, 2512, 281, 5065, 4320, 3159, 5050, 1951, 710, 2726, 2395, 1603, 4009, 2948, 530, 341, 761, 4113, 3819, 3731, 1788, 550, 1501, 5220, 241, 1579, 2438, 60, 686, 1652, 2330, 5037, 3660, 130, 4275, 1458, 3778, 3167, 157, 3396, 3213, 1046, 2893, 113, 1023, 3097, 1160, 1986, 2694, 4419, 522, 3648, 3211, 3612, 2895, 3065, 3506, 3571, 4431, 3362, 5446, 5, 90, 2250, 1767, 4131, 2017, 3328, 3581, 3457, 784, 1884, 3601, 4911, 5455, 1756, 72, 1132, 4257, 2919, 3664, 2197, 1342, 4067, 3481, 659, 3870, 852, 2441, 5511, 2347, 4230, 3857, 3262, 4599, 2339, 2928, 1028, 1378, 4965, 2561, 2513, 457, 3711, 1056, 3088, 821, 4774, 5400, 952, 1773, 578, 4034, 4551, 636, 5212, 5206, 5000, 935, 16, 4946, 5402, 5320, 4727, 1606, 4839, 1514, 1275, 4340, 2565, 754, 3936, 2865, 3289, 1589, 2567, 3578, 3287, 2556, 3202, 4101, 3535, 308, 5047, 5512, 2650, 477, 1109, 2287, 1302, 3772, 1420, 2015, 19, 5342, 2633, 4337, 1346, 2821, 3635, 2186, 4795, 629, 2143, 4692, 5174, 487, 2227, 1007, 122, 291, 4205, 4107, 2595, 2912, 5343, 1731, 4317, 845, 4628, 3245, 4850, 3823, 5292, 4437, 3661, 1035, 4416, 4826, 1679, 3223, 3054, 1212, 4203, 3591, 890, 2605, 4670, 2985, 5447, 5313, 4227, 3513, 5273, 5495, 2129, 2376, 3944, 4186, 2151, 3940, 5211, 478, 5356, 415, 2668, 1597, 3514, 1197, 3736, 5199, 216, 2540, 2257, 954, 643, 3988, 2054, 3785, 733, 1784, 425, 2622, 5148, 948, 2524, 3617, 3412, 3443, 2021, 1653, 3474, 623, 3091, 3705, 4403, 3505, 82, 3308, 3363, 5440, 5392, 4697, 1456, 4379, 728, 4354, 5334, 2380, 3703, 5435, 532, 2724, 5152, 4157, 3738, 4310, 1138, 3989, 1126, 511, 5081, 946, 208, 1646, 4180, 286, 248, 24, 2262, 1700, 1982, 4567, 3877, 3814, 3596, 820, 2528, 1760, 1290, 1320, 4520, 4471, 1200, 4663, 1664, 1810, 44, 4907, 404, 4501, 5398, 1657, 263, 2164, 1656, 2246, 250, 87, 5068, 4825, 344, 2597, 4380, 3478, 4530, 3388, 4434, 1974, 783, 3945, 4439, 4672, 724, 3049, 383, 3069, 2950, 5374, 4494, 2644, 1257, 3973, 2727, 503, 2575, 604, 1021, 843, 2811, 981, 3397, 4040, 2926, 3181, 2184, 1918, 408, 4050, 1632, 2233, 3848, 1291, 1642, 1113, 1627, 2803, 1687, 1601, 1460, 735, 5360, 1115, 1600, 2840, 704, 4019, 1904, 4639, 4420, 553, 3327, 5490, 5456, 2867, 3720, 1683, 1594, 3008, 3878, 5108, 3451, 732, 1570, 3131, 3688, 4638, 4377, 507, 3968, 3440, 3015, 3639, 1509, 4388, 5288, 449, 1179, 4462, 726, 5353, 360, 2464, 5119, 5221, 5304, 5120, 4097, 1870, 5170, 4977, 4465, 3060, 1808, 1878, 3079, 4394, 5395, 1008, 2596, 632, 2003, 1004, 3193, 184, 3817, 3127, 1232, 3058, 40, 1078, 978, 1, 2715, 1670, 412, 1718, 4701, 1468, 5049, 662, 3418, 1704, 2420, 4700, 1757, 3077, 2924, 1077, 4246, 5321, 5004, 3117, 5494, 810, 1708, 206, 2215, 5367, 4940, 4094, 4200, 1499, 4736, 4263, 997, 1329, 649, 1975, 1448, 1150, 1850, 5043, 730, 1899, 2747, 1206, 2414, 774, 5048, 5372, 2800, 5341, 5507, 1562, 3902, 5020, 1437, 2052, 801, 4384, 4667, 3815, 5420, 4855, 4104, 452, 2550, 1635, 3860, 4838, 288, 4918, 2722, 1835, 4943, 2982, 4502, 4218, 3102, 3470, 3048, 1308, 4891, 914, 4803, 2132, 4304, 2387, 1419, 3964, 4812, 4267, 221, 4698, 1846, 98, 790, 2969, 1834, 340, 2221, 4999, 775, 719, 4550, 768, 3709, 1661, 2283, 1333, 3640, 2074, 3990, 3096, 2411, 2060, 2766, 41, 3335, 3649, 4012, 3685, 700, 2408, 1465, 4629, 2794, 3134, 2841, 692, 1216, 2298, 4752, 2863, 3824, 5436, 3641, 1243, 2268, 5039, 2084, 1464, 1539, 4425, 2137, 838, 4626, 1106, 3996, 4299, 576, 2637, 4124, 196, 101, 3644, 3887, 3460, 1005, 3565, 111, 3948, 7, 875, 4053, 2405, 3033, 3338, 4768, 983, 2038, 2165, 52, 2515, 349, 5160, 4401, 167, 3499, 1944, 2549, 814, 3001, 4237, 1353, 1766, 4061, 1314, 1114, 793, 2765, 5481, 4914, 4342, 2149, 2870, 91, 539, 4314, 4262, 3810, 2995, 1980, 1155, 1722, 3894, 4327, 175, 5467, 1108, 5452, 4996, 5217, 4939, 4373, 5479, 1070, 857, 1096, 3899, 4460, 3386, 975, 4561, 2043, 5198, 71, 5087, 3073, 4153, 3366, 3676, 4121, 2899, 3818, 354, 5116, 2398, 5013, 3689, 2813, 5064, 4031, 891, 4832, 5318, 5268, 2307, 3455, 2443, 3538, 3728, 3882, 3890, 4343, 4446, 667, 3176, 4382, 1820, 5251, 1276, 5493, 4763, 4772, 4086, 4091, 306, 5009, 5278, 1204, 4060, 5044, 2485, 4054, 4528, 3361, 4105, 4481, 3953, 4602, 3827, 2499, 3665, 4289, 968, 4454, 352, 4449, 3683, 4491, 3484, 2010, 5188, 5128, 1547, 2535, 3119, 1743, 4458, 1368, 3869, 1453, 691, 3529, 2688, 3587, 2697, 201, 2319, 3615, 3992, 907, 4498, 551, 853, 1238, 182, 384, 3225, 432, 833, 1637, 1057, 1550, 1576, 1055, 5203, 5186, 4142, 3670, 826, 4970, 2643, 1164, 59, 1643, 5448, 2097, 2018, 2189, 3760, 5497, 2731, 1409, 2602, 2112, 1922, 3324, 3423, 4266, 4044, 5002, 495, 1269, 1387, 5194, 1942, 971, 4616, 2855, 716, 705, 3820, 529, 2547, 4665, 3216, 4852, 5058, 1966, 5060, 2472, 453, 1459, 1397, 4161, 2311, 1744, 1669, 2331, 3959, 270, 2322, 5241, 5428, 516, 2607, 3494, 4909, 4051, 2349, 5338, 2976, 4728, 4370, 3168, 3544, 4264, 4645, 15, 3331, 4068, 1149, 2461, 4196, 4961, 3691, 3858, 3083, 3863, 2580, 567, 1491, 3047, 1889, 2062, 3942, 492, 3411, 5480, 5027, 5466, 1398, 4648, 3201, 3998, 3378, 1736, 5287, 3835, 388, 702, 2192, 4615, 974, 4231, 5385, 5290, 1447, 370, 601, 2838, 3655, 5426, 1525, 4547, 4649, 4447, 1137, 2454, 4724, 5214, 861, 5055, 1985, 533, 4279, 3154, 2379, 963, 2024, 145, 5407, 398, 3861, 1764]\n",
            "[0, 8, 11, 20, 22, 28, 30, 31, 32, 35, 36, 42, 47, 51, 54, 55, 61, 65, 69, 73, 75, 78, 80, 81, 84, 85, 86, 88, 95, 96, 97, 99, 102, 104, 105, 108, 117, 123, 128, 136, 139, 140, 143, 144, 146, 147, 155, 162, 165, 177, 180, 189, 191, 194, 197, 202, 224, 225, 228, 232, 238, 239, 264, 266, 271, 272, 274, 284, 285, 287, 289, 292, 293, 296, 297, 305, 310, 312, 320, 331, 339, 358, 362, 364, 377, 379, 380, 386, 391, 392, 393, 395, 397, 402, 406, 409, 417, 427, 431, 435, 437, 441, 442, 445, 451, 456, 460, 469, 470, 472, 473, 481, 483, 489, 490, 494, 505, 506, 514, 515, 518, 537, 558, 587, 589, 592, 593, 595, 607, 608, 619, 621, 630, 638, 639, 640, 641, 642, 648, 666, 675, 676, 680, 696, 703, 708, 711, 715, 722, 723, 725, 729, 741, 742, 743, 752, 757, 758, 769, 773, 776, 779, 785, 788, 798, 805, 809, 824, 829, 830, 831, 836, 850, 851, 854, 860, 862, 866, 868, 869, 870, 880, 882, 896, 899, 901, 902, 910, 911, 915, 917, 922, 930, 933, 936, 939, 956, 957, 960, 961, 989, 995, 998, 1011, 1017, 1019, 1027, 1029, 1039, 1040, 1041, 1043, 1049, 1052, 1060, 1061, 1062, 1063, 1067, 1069, 1075, 1080, 1081, 1086, 1088, 1089, 1090, 1091, 1102, 1103, 1105, 1107, 1116, 1121, 1123, 1128, 1131, 1136, 1144, 1146, 1147, 1156, 1157, 1166, 1168, 1177, 1181, 1187, 1190, 1191, 1194, 1202, 1205, 1214, 1223, 1224, 1226, 1234, 1239, 1244, 1249, 1254, 1258, 1259, 1272, 1278, 1279, 1293, 1300, 1301, 1312, 1318, 1324, 1326, 1327, 1337, 1339, 1344, 1357, 1366, 1367, 1369, 1375, 1376, 1381, 1384, 1389, 1391, 1396, 1400, 1403, 1404, 1408, 1415, 1422, 1433, 1446, 1452, 1455, 1461, 1466, 1487, 1493, 1512, 1513, 1515, 1521, 1524, 1526, 1534, 1548, 1549, 1551, 1552, 1560, 1578, 1587, 1596, 1598, 1611, 1612, 1615, 1617, 1618, 1619, 1622, 1639, 1645, 1648, 1649, 1655, 1658, 1662, 1665, 1668, 1676, 1678, 1688, 1698, 1707, 1726, 1727, 1739, 1740, 1747, 1748, 1754, 1771, 1774, 1775, 1776, 1778, 1782, 1786, 1787, 1816, 1817, 1830, 1832, 1833, 1849, 1854, 1855, 1856, 1858, 1859, 1860, 1868, 1881, 1883, 1897, 1898, 1900, 1901, 1911, 1916, 1919, 1930, 1932, 1937, 1938, 1941, 1946, 1957, 1969, 1970, 1971, 1976, 1981, 1988, 1992, 1994, 2007, 2008, 2012, 2013, 2022, 2031, 2034, 2035, 2050, 2061, 2064, 2069, 2072, 2073, 2081, 2089, 2094, 2095, 2103, 2104, 2106, 2114, 2115, 2117, 2118, 2128, 2130, 2133, 2138, 2140, 2142, 2150, 2155, 2158, 2159, 2177, 2181, 2182, 2196, 2204, 2205, 2209, 2213, 2217, 2218, 2222, 2223, 2244, 2245, 2264, 2267, 2273, 2274, 2275, 2292, 2294, 2299, 2313, 2327, 2328, 2332, 2338, 2352, 2353, 2357, 2363, 2367, 2373, 2374, 2375, 2381, 2382, 2391, 2394, 2396, 2397, 2400, 2402, 2416, 2419, 2421, 2428, 2436, 2455, 2458, 2459, 2463, 2466, 2471, 2480, 2482, 2486, 2492, 2494, 2495, 2503, 2505, 2507, 2509, 2522, 2523, 2527, 2529, 2532, 2534, 2543, 2544, 2554, 2562, 2563, 2566, 2570, 2578, 2585, 2600, 2609, 2614, 2621, 2623, 2653, 2658, 2659, 2666, 2673, 2678, 2682, 2692, 2693, 2695, 2699, 2702, 2703, 2705, 2708, 2713, 2714, 2716, 2725, 2729, 2733, 2738, 2739, 2740, 2748, 2763, 2772, 2775, 2797, 2798, 2799, 2801, 2806, 2815, 2823, 2832, 2846, 2847, 2852, 2853, 2857, 2871, 2874, 2877, 2879, 2884, 2885, 2888, 2891, 2894, 2900, 2904, 2909, 2911, 2916, 2923, 2929, 2933, 2935, 2937, 2945, 2957, 2959, 2966, 2968, 2974, 2980, 2984, 2986, 2989, 2993, 2998, 2999, 3016, 3020, 3022, 3029, 3030, 3037, 3038, 3040, 3042, 3045, 3046, 3057, 3064, 3067, 3068, 3075, 3076, 3082, 3086, 3089, 3093, 3094, 3095, 3098, 3106, 3107, 3109, 3118, 3122, 3132, 3136, 3139, 3143, 3147, 3156, 3163, 3164, 3170, 3171, 3173, 3177, 3178, 3182, 3184, 3186, 3192, 3196, 3203, 3205, 3208, 3209, 3215, 3217, 3218, 3219, 3220, 3226, 3234, 3236, 3238, 3254, 3258, 3267, 3269, 3272, 3275, 3277, 3278, 3279, 3284, 3293, 3294, 3295, 3301, 3305, 3314, 3315, 3320, 3322, 3333, 3337, 3340, 3341, 3347, 3348, 3351, 3352, 3355, 3356, 3357, 3359, 3360, 3364, 3369, 3371, 3373, 3391, 3393, 3402, 3410, 3413, 3417, 3422, 3424, 3425, 3431, 3435, 3445, 3446, 3447, 3449, 3452, 3454, 3459, 3461, 3468, 3472, 3477, 3480, 3491, 3498, 3502, 3504, 3508, 3519, 3525, 3536, 3541, 3542, 3543, 3549, 3550, 3554, 3557, 3558, 3564, 3583, 3584, 3590, 3604, 3610, 3616, 3620, 3645, 3650, 3651, 3653, 3654, 3656, 3666, 3680, 3681, 3693, 3696, 3701, 3704, 3713, 3732, 3734, 3735, 3747, 3751, 3761, 3767, 3773, 3791, 3792, 3796, 3797, 3802, 3804, 3813, 3821, 3822, 3830, 3831, 3833, 3837, 3838, 3841, 3844, 3856, 3864, 3873, 3880, 3889, 3891, 3897, 3898, 3910, 3914, 3915, 3918, 3919, 3925, 3933, 3937, 3946, 3949, 3951, 3955, 3956, 3971, 3975, 3977, 3979, 3985, 3991, 3993, 4000, 4005, 4010, 4018, 4022, 4025, 4027, 4029, 4037, 4038, 4039, 4041, 4052, 4064, 4065, 4066, 4070, 4077, 4078, 4093, 4095, 4108, 4112, 4118, 4123, 4129, 4136, 4137, 4143, 4149, 4151, 4171, 4175, 4181, 4185, 4188, 4190, 4191, 4198, 4201, 4202, 4207, 4210, 4223, 4233, 4236, 4238, 4240, 4245, 4260, 4272, 4276, 4277, 4278, 4285, 4286, 4292, 4295, 4300, 4301, 4302, 4305, 4312, 4315, 4324, 4325, 4328, 4336, 4346, 4353, 4357, 4358, 4361, 4368, 4378, 4386, 4391, 4395, 4396, 4399, 4400, 4405, 4429, 4448, 4456, 4457, 4466, 4468, 4472, 4478, 4480, 4484, 4485, 4490, 4495, 4504, 4507, 4509, 4511, 4513, 4525, 4538, 4539, 4540, 4546, 4549, 4556, 4557, 4559, 4564, 4565, 4568, 4590, 4593, 4596, 4606, 4609, 4614, 4618, 4621, 4624, 4627, 4631, 4633, 4641, 4643, 4650, 4656, 4657, 4659, 4673, 4674, 4680, 4684, 4689, 4691, 4694, 4703, 4705, 4707, 4713, 4717, 4718, 4721, 4726, 4731, 4732, 4735, 4738, 4741, 4753, 4759, 4761, 4765, 4766, 4767, 4777, 4789, 4791, 4798, 4800, 4801, 4802, 4804, 4811, 4814, 4821, 4822, 4823, 4841, 4842, 4844, 4847, 4851, 4860, 4861, 4862, 4864, 4868, 4872, 4876, 4879, 4890, 4892, 4896, 4898, 4903, 4913, 4922, 4923, 4933, 4937, 4948, 4951, 4952, 4955, 4957, 4958, 4963, 4964, 4972, 4973, 4981, 4984, 4991, 4995, 5006, 5022, 5023, 5033, 5038, 5042, 5046, 5052, 5053, 5063, 5079, 5083, 5085, 5089, 5090, 5099, 5101, 5107, 5111, 5114, 5123, 5125, 5127, 5133, 5134, 5137, 5138, 5146, 5154, 5156, 5175, 5179, 5181, 5185, 5195, 5196, 5197, 5200, 5202, 5204, 5205, 5210, 5219, 5224, 5228, 5230, 5231, 5236, 5243, 5245, 5252, 5253, 5254, 5256, 5258, 5261, 5267, 5272, 5276, 5285, 5289, 5293, 5294, 5305, 5306, 5314, 5319, 5322, 5327, 5336, 5346, 5364, 5366, 5368, 5371, 5378, 5383, 5386, 5387, 5391, 5394, 5396, 5409, 5410, 5411, 5427, 5429, 5433, 5439, 5443, 5445, 5458, 5461, 5462, 5468, 5477, 5483, 5487, 5498, 5499, 5500, 5503, 5509, 5510, 5517]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Multimodal Baseline Model"
      ],
      "metadata": {
        "id": "gj8p3uDbk9q0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Architecture"
      ],
      "metadata": {
        "id": "SCyYqusI-Wt1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MultimodalClassifier_baseline(nn.Module):\n",
        "    def __init__(self, pretrained_acoustic_dict, pretr_model_lex, hidden_dim, num_classes, input_dim_a=65, input_dim_l=768, num_layers=2, device=torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")):\n",
        "        super().__init__()\n",
        "\n",
        "        model_acoustic = BLSTM_avg_pooling(input_dim_a, hidden_dim, num_layers, num_classes).to(device)\n",
        "        model_acoustic.load_state_dict(pretrained_acoustic_dict)\n",
        "\n",
        "        self.blstm_acoustic = model_acoustic.rnn\n",
        "        self.blstm_lex = pretr_model_lex.rnn\n",
        "\n",
        "        self.fc_mm = torch.nn.Linear(hidden_dim * 4, num_classes) \n",
        "        self.fc_mm.weight.data = torch.cat((pretrained_acoustic_dict['fc.weight'], pretr_model_lex.state_dict()['fc.weight']), dim=1)\n",
        "        self.fc_mm.bias.data = pretrained_acoustic_dict['fc.bias'] + pretr_model_lex.state_dict()['fc.bias']\n",
        "\n",
        "    def forward(self, x_acoustic, lengths_acoustic, x_lex, lengths_lex):\n",
        "        # Pack the padded sequence\n",
        "        x_acoustic = torch.nn.utils.rnn.pack_padded_sequence(x_acoustic, lengths_acoustic,\n",
        "                                                             batch_first=True, enforce_sorted=False)\n",
        "        x_lex = torch.nn.utils.rnn.pack_padded_sequence(x_lex, lengths_lex,\n",
        "                                                        batch_first=True, enforce_sorted=False)\n",
        "\n",
        "        output_acoustic,_ = self.blstm_acoustic(x_acoustic)\n",
        "        output_lex,_ = self.blstm_lex(x_lex)\n",
        "\n",
        "        # Unpack the packed sequence\n",
        "        output_acoustic, _ = torch.nn.utils.rnn.pad_packed_sequence(output_acoustic, batch_first=True)\n",
        "        output_lex, _ = torch.nn.utils.rnn.pad_packed_sequence(output_lex, batch_first=True)\n",
        "\n",
        "        # Compute the average across the sequence dimension (axis=1)\n",
        "        output_acoustic = torch.mean(output_acoustic, dim=1)\n",
        "        output_lex = torch.mean(output_lex, dim=1)\n",
        "\n",
        "        # Concatenate the two outputs\n",
        "        output_concat = torch.cat((output_acoustic, output_lex), dim=1)\n",
        "\n",
        "        # Pass the concatenated output through the linear layer for classification\n",
        "        output = self.fc_mm(output_concat)\n",
        "        return output"
      ],
      "metadata": {
        "id": "7taijOMW952l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training and Testing"
      ],
      "metadata": {
        "id": "YSyWVlyh952n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset_mm\n",
        "test_dataset_mm"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2810a4f6-07c3-4a46-df76-3f3a6c320bca",
        "id": "YalMH_M9952n"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch.utils.data.dataloader.DataLoader at 0x7f3bceedae50>"
            ]
          },
          "metadata": {},
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Set up the training settings\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)\n",
        "# Initialize the hyperparameters\n",
        "learning_rate = 1e-4 \n",
        "input_dim_a = 65\n",
        "input_dim_l = 768 # dataset_lex[0][0].shape[-1] #  = 768 (the size of the word embeddings)\n",
        "hidden_dim = 128\n",
        "num_layers = 2\n",
        "num_classes = len(np.unique(iemocap_dataset.y))\n",
        "attention_dim = 64"
      ],
      "metadata": {
        "id": "kUeyhHak952l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the pre-trained model\n",
        "acoustic_model_path = os.path.join(main_dir,\"model_acoustic_II.pth\")\n",
        "pretrained_acoustic_dict = torch.load(acoustic_model_path,map_location=torch.device(device))\n",
        "# Load the pre-trained model\n",
        "lex_model_path =os.path.join(main_dir,\"model_lex_II.pth\")\n",
        "pretr_model_lex = torch.load(lex_model_path,map_location=torch.device(device))"
      ],
      "metadata": {
        "id": "GDQAg-WA952m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_bmm = MultimodalClassifier_baseline(pretrained_acoustic_dict, pretr_model_lex, hidden_dim,\n",
        "                                          num_classes,input_dim_a,input_dim_l,num_layers)\n",
        "print(model_bmm)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model_bmm.parameters(), lr=learning_rate)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9382e379-9525-424d-9c26-50574598f313",
        "id": "CUNY4bTa952m"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MultimodalClassifier_baseline(\n",
            "  (blstm_acoustic): LSTM(65, 128, num_layers=2, batch_first=True, bidirectional=True)\n",
            "  (blstm_lex): LSTM(768, 128, num_layers=2, batch_first=True, bidirectional=True)\n",
            "  (fc_mm): Linear(in_features=512, out_features=4, bias=True)\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "file_name = 'model_bmm'\n",
        "save_path =os.path.join(main_dir,\"model_mm/\")"
      ],
      "metadata": {
        "id": "h2tUQNMuiUy0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "YfijBzfp952n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_epochs = 5\n",
        "\n",
        "from tqdm.notebook import tqdm\n",
        "from sklearn.metrics import accuracy_score,balanced_accuracy_score\n",
        "from collections import defaultdict\n",
        "\n",
        "# Define a function to compute class-wise accuracy\n",
        "def class_wise_accuracy(true_labels, predicted_labels, num_classes):\n",
        "    class_correct = [0] * num_classes\n",
        "    class_total = [0] * num_classes\n",
        "    \n",
        "    for t, p in zip(true_labels, predicted_labels):\n",
        "        class_correct[t] += (t == p)\n",
        "        class_total[t] += 1\n",
        "        \n",
        "    return [correct / total if total > 0 else 0 for correct, total in zip(class_correct, class_total)]\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(2,8):\n",
        "\n",
        "    model_bmm.train()   #----<----\n",
        "    \n",
        "    train_loss = 0\n",
        "    train_iter = 0\n",
        "    train_labels = []\n",
        "    train_preds = []\n",
        "    \n",
        "    loop = tqdm(enumerate(train_dataset_mm), total=len(train_dataset_mm), leave=True)\n",
        "    for i, (feat_a,seq_size_a,feat_l,seq_size_l,labels) in loop:\n",
        "        feat_a = feat_a.to(device)\n",
        "        feat_l = feat_l.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        # Forward pass\n",
        "\n",
        "        outputs = model_bmm(feat_a,seq_size_a,feat_l,seq_size_l)  # ----<----\n",
        "\n",
        "        loss = criterion(outputs, labels)\n",
        "        train_loss += loss.item()\n",
        "        train_iter += 1\n",
        "\n",
        "        # Backward pass and optimization\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # Update progress bar\n",
        "        loop.set_description(f\"Epoch [{epoch + 1}/{num_epochs}]\")\n",
        "        loop.set_postfix(loss=loss.item())\n",
        "\n",
        "        # Store labels and predictions for training accuracy\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        train_labels.extend(labels.cpu().numpy())\n",
        "        train_preds.extend(predicted.cpu().numpy())\n",
        "\n",
        "    # Evaluate the model on the test set\n",
        "    model_bmm.eval() #----<----\n",
        "    test_loss = 0\n",
        "    test_iter = 0\n",
        "    test_labels = []\n",
        "    test_preds = []\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        for feat_a,seq_size_a,feat_l,seq_size_l,labels in test_dataset_mm:\n",
        "            feat_a = feat_a.to(device)\n",
        "            feat_l = feat_l.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            outputs = model_bmm(feat_a,seq_size_a,feat_l,seq_size_l)  #  ----<----\n",
        "            loss = criterion(outputs, labels)\n",
        "            test_loss += loss.item()\n",
        "            test_iter += 1\n",
        "            \n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            test_labels.extend(labels.cpu().numpy())\n",
        "            test_preds.extend(predicted.cpu().numpy())\n",
        "\n",
        "    avg_train_loss = train_loss / train_iter\n",
        "    avg_test_loss = test_loss / test_iter\n",
        "    weighted_train_accuracy = accuracy_score(train_labels, train_preds)\n",
        "    unweighted_train_accuracy = balanced_accuracy_score(train_labels, train_preds)\n",
        "    weighted_test_accuracy = accuracy_score(test_labels, test_preds)\n",
        "    unweighted_test_accuracy = balanced_accuracy_score(test_labels, test_preds)\n",
        "    train_class_accuracies = class_wise_accuracy(train_labels, train_preds, num_classes)\n",
        "    test_class_accuracies = class_wise_accuracy(test_labels, test_preds, num_classes)\n",
        "\n",
        "    print(f\"Epoch[{epoch + 1}] Avg Train Loss: {avg_train_loss:.4f}, Weighted Train Accuracy: {100 * weighted_train_accuracy:.2f}%, Unweighted Train Accuracy: {100 * unweighted_train_accuracy:.2f}%\")\n",
        "    print(f\" Avg Test Loss: {avg_test_loss:.4f}, Weighted Test Accuracy: {100 * weighted_test_accuracy:.2f}%, Unweighted Test Accuracy: {100 * unweighted_test_accuracy:.2f}%\")\n",
        "    print(f\"Train Class Accuracies: {', '.join([f'Class {i}: {100 * acc:.2f}%' for i, acc in enumerate(train_class_accuracies)])}\")\n",
        "    print(f\"Test Class Accuracies: {', '.join([f'Class {i}: {100 * acc:.2f}%' for i, acc in enumerate(test_class_accuracies)])}\")\n",
        "    file_path_epoch = os.path.join(save_path, file_name + str(epoch+1)+'.pth')\n",
        "    torch.save(model_bmm, file_path_epoch)"
      ],
      "metadata": {
        "id": "sVUne_n_952o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Baseline Multimodal Results\n",
        "**Train Results:**\n",
        "\n",
        "| Model | Train Loss | Weighted Accuracy | Unweighted Accuracy | Angry Acc. | Happy Acc. | Neutral Acc. | Sad Acc. |\n",
        "|-------|-----------|------------------|---------------------|------------|------------|--------------|----------|\n",
        "| B-MM (epochs: 7) | 0.4708 | 83.99% | 84.63% | 89.65% | 80.24% | 82.46% | 86.17% |\n",
        "\n",
        "Note: The categorical names used are: Angry, Happy, Neutral, and Sad.\n",
        "\n",
        "**Test Results:**\n",
        "\n",
        "| Model | Test Loss | Weighted Accuracy | Unweighted Accuracy | Angry Acc. | Happy Acc. | Neutral Acc. | Sad Acc. |\n",
        "|-------|-----------|------------------|---------------------|------------|------------|--------------|----------|\n",
        "| B-MM (epochs: 7) | 0.8217 | 72.46% | 72.24% | 72.90% | 72.81% | 73.19% | 70.05% |\n",
        "\n",
        "Note: The categorical names used are: Angry, Happy, Neutral, and Sad."
      ],
      "metadata": {
        "id": "iAJ3qPO6LPpx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## MultiModal Classifier using GMU (Gated Multimodal Unit) Arevalo et al.\n",
        "Modality-based attention is a mechanism that aims to prioritize one modality over another based on the relevance of the input features for a specific task, such as capturing emotions. The bimodal Gated Multimodal Unit (GMU) cell proposed by Arevalo et al. (2017) is used to achieve this behavior. The GMU cell consists of a set of equations that incorporate a complementary mechanism over the modalities, allowing the model to prioritize one modality when necessary.\n",
        "\n",
        "The GMU equations can be explained as follows:\n",
        "\n",
        "$h_a = tanh(W_ax_a + b_a)$: This equation calculates the hidden acoustic vector ($h_a$) by multiplying the acoustic input vector ($x_a$) with a weight matrix ($W_a$) and adding a bias term ($b_a$). The result is passed through the hyperbolic tangent ($tanh$) activation function.\n",
        "\n",
        "$h_l = tanh(W_lx_l + b_l)$: This equation calculates the hidden lexical vector ($h_l$) by multiplying the lexical input vector ($x_l$) with a weight matrix ($W_l$) and adding a bias term ($b_l$). The result is passed through the hyperbolic tangent ($tanh$) activation function.\n",
        "\n",
        "$z = \\sigma(W_z[x_a, x_l] + b_z)$: This equation computes the gating mechanism ($z$) by concatenating the acoustic and lexical input vectors ($x_a$ and $x_l$) and multiplying them with a weight matrix ($W_z$) and adding a bias term ($b_z$). The result is passed through the sigmoid activation function ($\\sigma$) to obtain values between 0 and 1.\n",
        "\n",
        "$h = z * h_a + (1 − z) * h_l$: This final equation computes the output hidden vector ($h$) by taking the element-wise product of the gating mechanism ($z$) and the hidden acoustic vector ($h_a$) and adding it to the element-wise product of $(1 − z)$ and the hidden lexical vector $(h_l)$. This process allows the model to prioritize one modality over the other based on the input features.\n",
        "\n",
        "In summary, the modality-based attention mechanism enables a model to focus on either the acoustic or lexical input features when determining the output, depending on the relevance of each modality for the task at hand. The bimodal GMU cell achieves this by incorporating a complementary mechanism over the modalities, allowing the model to prioritize one modality over another when necessary.\n",
        "\n",
        "$h_a,\\ h_l \\ and \\ z$ are of same dimension $d_g$\n",
        "\n",
        "The sizes of the hidden acoustic and lexical vectors and the gating vector z depend on the design choices made during the implementation of the GMU cell. The number of hidden units in the model is a tunable hyperparameter, and the gating vector z has the same size as the hidden vectors to enable element-wise multiplication for combining the contributions from both modalities."
      ],
      "metadata": {
        "id": "ipHGXAjHMm2H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model Architecture"
      ],
      "metadata": {
        "id": "7lJ6cFGhAawW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class ContextBasedAttention_mm(nn.Module):\n",
        "    def __init__(self, hidden_dim, attention_dim):\n",
        "        super().__init__()\n",
        "        self.wh = nn.Linear(hidden_dim, attention_dim)\n",
        "        self.v = nn.Parameter(torch.rand(attention_dim, 1))\n",
        "        self.tanh = nn.Tanh()\n",
        "        self.softmax = nn.Softmax(dim=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        hi = self.wh(x)\n",
        "        ei = self.tanh(hi).matmul(self.v)\n",
        "        ai = self.softmax(ei)\n",
        "        z = torch.sum(ai * x, dim=1)\n",
        "        return z,ai\n",
        "\n",
        "class GMU(nn.Module):\n",
        "    def __init__(self, input_dim_a, input_dim_l, hidden_out_dim):\n",
        "        super().__init__()\n",
        "\n",
        "        self.fc_acoustic = nn.Linear(input_dim_a, hidden_out_dim)\n",
        "        self.fc_lex = nn.Linear(input_dim_l, hidden_out_dim)\n",
        "        self.fc_gate = nn.Linear(input_dim_a + input_dim_l, hidden_out_dim)\n",
        "\n",
        "    def forward(self, output_acoustic, output_lex):\n",
        "        ha = torch.tanh(self.fc_acoustic(output_acoustic))\n",
        "        hl = torch.tanh(self.fc_lex(output_lex))\n",
        "        z = torch.sigmoid(self.fc_gate(torch.cat((output_acoustic, output_lex), dim=1)))\n",
        "\n",
        "        h = z * ha + (1 - z) * hl\n",
        "        return h,z\n",
        "\n",
        "class MultimodalClassifier_GMU(nn.Module):\n",
        "    def __init__(self, pretrained_acoustic_dict, pretr_model_lex, rnn_hidden_dim,gmu_out_dim, num_classes,\n",
        "                 attention_dim, input_dim_a=65, input_dim_l=768, num_layers=2,\n",
        "                 device=torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")):\n",
        "        super().__init__()\n",
        "        # the BLSTMs from the pre-trained lexical model\n",
        "        pretr_model_acoustic = BLSTMWithContextBasedAttention(input_dim_a, rnn_hidden_dim,num_layers,\n",
        "                                                              attention_dim, num_classes).to(device)\n",
        "        pretr_model_acoustic.load_state_dict(pretrained_acoustic_dict)\n",
        "        self.blstm_acoustic = pretr_model_acoustic.rnn\n",
        "        # the BLSTMs from the pre-trained lexical model\n",
        "        self.blstm_lex = pretr_model_lex.rnn\n",
        "        # LLattention (low level attention)\n",
        "        # self.LL_attention_l = pretr_model_lex.attention\n",
        "        self.LL_attention_a = ContextBasedAttention_mm(rnn_hidden_dim*2, attention_dim)\n",
        "        LL_attention_a.load_state_dict(pretr_model_acoustic.attention.state_dict())\n",
        "        self.LL_attention_l = ContextBasedAttention_mm(rnn_hidden_dim*2, attention_dim)\n",
        "        self.LL_attention_l.load_state_dict(pretr_model_lex.attention.state_dict())\n",
        "        \n",
        "        # HLfusion (high level), GMU attention\n",
        "        self.gmu = GMU(rnn_hidden_dim*2, rnn_hidden_dim*2, gmu_out_dim)\n",
        "        # linear softmax classification\n",
        "        self.fc_mm = torch.nn.Linear(gmu_out_dim, num_classes) \n",
        "\n",
        "    def forward(self, x_acoustic, lengths_acoustic, x_lex, lengths_lex):\n",
        "        # Pack the padded sequence\n",
        "        x_acoustic = torch.nn.utils.rnn.pack_padded_sequence(x_acoustic, lengths_acoustic,\n",
        "                                                             batch_first=True, enforce_sorted=False)\n",
        "        x_lex = torch.nn.utils.rnn.pack_padded_sequence(x_lex, lengths_lex,\n",
        "                                                        batch_first=True, enforce_sorted=False)\n",
        "\n",
        "        output_acoustic,_ = self.blstm_acoustic(x_acoustic)\n",
        "        output_lex,_ = self.blstm_lex(x_lex)\n",
        "\n",
        "        # Unpack the packed sequence\n",
        "        output_acoustic, _ = torch.nn.utils.rnn.pad_packed_sequence(output_acoustic, batch_first=True)\n",
        "        output_lex, _ = torch.nn.utils.rnn.pad_packed_sequence(output_lex, batch_first=True)\n",
        "\n",
        "        # Compute the average across the sequence dimension (axis=1)\n",
        "        output_acoustic, acoustic_attention = self.LL_attention_a(output_acoustic) # torch.mean(output_acoustic, dim=1)\n",
        "        output_lex, lexical_attention = self.LL_attention_l(output_lex) # torch.mean(output_lex, dim=1)\n",
        "\n",
        "        # GMU attention mechanism\n",
        "        h,modality_attention = self.gmu(output_acoustic, output_lex)\n",
        "\n",
        "        # Pass the combined output through the linear layer for classification\n",
        "        output = self.fc_mm(h)\n",
        "        return output,acoustic_attention,lexical_attention,modality_attention\n"
      ],
      "metadata": {
        "id": "mOGDltXfLPcG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training MMMF-ER"
      ],
      "metadata": {
        "id": "rkHshh5hkGBu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "# Load the pre-trained model\n",
        "acoustic_model_path = os.path.join(main_dir, \"model_acoustic_III.pth\")\n",
        "pretrained_acoustic_dict = torch.load(acoustic_model_path,map_location=torch.device(device))\n",
        "\n",
        "# Load the pre-trained model\n",
        "lex_model_path = os.path.join(main_dir,\"model_lex_III.pth\")\n",
        "pretr_model_lex = torch.load(lex_model_path,map_location=torch.device(device))\n",
        "print(device)"
      ],
      "metadata": {
        "id": "eOk7QZZ-fUPC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1c70e7d8-888e-4006-fd2b-8fa00a95d75d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cpu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Set up the training settings\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)\n",
        "# Initialize the hyperparameters\n",
        "learning_rate = 1e-4 \n",
        "input_dim_a = 65\n",
        "input_dim_l = 768 # dataset_lex[0][0].shape[-1] #  = 768 (the size of the word embeddings)\n",
        "rnn_hidden_dim = 128\n",
        "num_layers = 2\n",
        "num_classes = len(np.unique(iemocap_dataset.y))\n",
        "attention_dim = 64\n",
        "gmu_out_dim = 128"
      ],
      "metadata": {
        "id": "j2yWJaoeBBi8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_mm_mla = MultimodalClassifier_GMU(pretrained_acoustic_dict, pretr_model_lex,\n",
        "                                        rnn_hidden_dim,gmu_out_dim,num_classes,attention_dim).to(device)\n",
        "save_path = os.path.join(main_dir,'model_mm/') \n",
        "file_name = 'model_mm_mla'\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model_mm_mla.parameters(), lr=learning_rate)"
      ],
      "metadata": {
        "id": "eA_1fHZcj60m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset_mm\n",
        "test_dataset_mm"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1ce9de72-6df0-4b88-b2c6-7a765250d824",
        "id": "RtiHfvDJkD2S"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch.utils.data.dataloader.DataLoader at 0x7fa183bafb50>"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "num_epochs = 2\n",
        "\n",
        "from tqdm.notebook import tqdm\n",
        "from sklearn.metrics import accuracy_score,balanced_accuracy_score\n",
        "from collections import defaultdict\n",
        "\n",
        "# Define a function to compute class-wise accuracy\n",
        "def class_wise_accuracy(true_labels, predicted_labels, num_classes):\n",
        "    class_correct = [0] * num_classes\n",
        "    class_total = [0] * num_classes\n",
        "    \n",
        "    for t, p in zip(true_labels, predicted_labels):\n",
        "        class_correct[t] += (t == p)\n",
        "        class_total[t] += 1\n",
        "        \n",
        "    return [correct / total if total > 0 else 0 for correct, total in zip(class_correct, class_total)]\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(num_epochs):\n",
        "\n",
        "    model_mm_mla.train()   #----<----\n",
        "    \n",
        "    train_loss = 0\n",
        "    train_iter = 0\n",
        "    train_labels = []\n",
        "    train_preds = []\n",
        "    \n",
        "    loop = tqdm(enumerate(train_dataset_mm), total=len(train_dataset_mm), leave=True)\n",
        "    for i, (feat_a,seq_size_a,feat_l,seq_size_l,labels) in loop:\n",
        "        feat_a = feat_a.to(device)\n",
        "        feat_l = feat_l.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        # Forward pass\n",
        "\n",
        "        outputs,_,_,_ = model_mm_mla(feat_a,seq_size_a,feat_l,seq_size_l)  # ----<----\n",
        "\n",
        "        loss = criterion(outputs, labels)\n",
        "        train_loss += loss.item()\n",
        "        train_iter += 1\n",
        "\n",
        "        # Backward pass and optimization\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # Update progress bar\n",
        "        loop.set_description(f\"Epoch [{epoch + 1}/{num_epochs}]\")\n",
        "        loop.set_postfix(loss=loss.item())\n",
        "\n",
        "        # Store labels and predictions for training accuracy\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        train_labels.extend(labels.cpu().numpy())\n",
        "        train_preds.extend(predicted.cpu().numpy())\n",
        "\n",
        "    # Evaluate the model on the test set\n",
        "    model_mm_mla.eval() #----<----\n",
        "    test_loss = 0\n",
        "    test_iter = 0\n",
        "    test_labels = []\n",
        "    test_preds = []\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        for feat_a,seq_size_a,feat_l,seq_size_l,labels in test_dataset_mm:\n",
        "            feat_a = feat_a.to(device)\n",
        "            feat_l = feat_l.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            outputs,_,_,_ = model_mm_mla(feat_a,seq_size_a,feat_l,seq_size_l)  #  ----<----\n",
        "            loss = criterion(outputs, labels)\n",
        "            test_loss += loss.item()\n",
        "            test_iter += 1\n",
        "            \n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            test_labels.extend(labels.cpu().numpy())\n",
        "            test_preds.extend(predicted.cpu().numpy())\n",
        "\n",
        "    avg_train_loss = train_loss / train_iter\n",
        "    avg_test_loss = test_loss / test_iter\n",
        "    weighted_train_accuracy = accuracy_score(train_labels, train_preds)\n",
        "    unweighted_train_accuracy = balanced_accuracy_score(train_labels, train_preds)\n",
        "    weighted_test_accuracy = accuracy_score(test_labels, test_preds)\n",
        "    unweighted_test_accuracy = balanced_accuracy_score(test_labels, test_preds)\n",
        "    train_class_accuracies = class_wise_accuracy(train_labels, train_preds, num_classes)\n",
        "    test_class_accuracies = class_wise_accuracy(test_labels, test_preds, num_classes)\n",
        "\n",
        "    print(f\"Epoch[{epoch + 1}] Avg Train Loss: {avg_train_loss:.4f}, Weighted Train Accuracy: {100 * weighted_train_accuracy:.2f}%, Unweighted Train Accuracy: {100 * unweighted_train_accuracy:.2f}%\")\n",
        "    print(f\" Avg Test Loss: {avg_test_loss:.4f}, Weighted Test Accuracy: {100 * weighted_test_accuracy:.2f}%, Unweighted Test Accuracy: {100 * unweighted_test_accuracy:.2f}%\")\n",
        "    print(f\"Train Class Accuracies: {', '.join([f'Class {i}: {100 * acc:.2f}%' for i, acc in enumerate(train_class_accuracies)])}\")\n",
        "    print(f\"Test Class Accuracies: {', '.join([f'Class {i}: {100 * acc:.2f}%' for i, acc in enumerate(test_class_accuracies)])}\")\n",
        "    file_path_epoch = os.path.join(save_path,file_name + str(epoch+1)+'.pth') \n",
        "    torch.save(model_mm_mla, file_path_epoch)   #  ----<----\n",
        "    # torch.save(model_mm_mla, 'model_mm_mla'+ str(epoch+1)+ '.pth') "
      ],
      "metadata": {
        "id": "3_ZF5v2ckD2T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "file_path_epoch = os.path.join(save_path,file_name + str(epoch+1)+'.pth') \n",
        "torch.save(model_mm_mla, file_path_epoch)   #  ----<----"
      ],
      "metadata": {
        "id": "qeAqx5KHwVxA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### MMMA Model Results\n",
        "\n",
        "**Train Results:**\n",
        "\n",
        "| Model | Train Loss | Weighted Accuracy | Unweighted Accuracy | Angry Acc. | Happy Acc. | Neutral Acc. | Sad Acc. |\n",
        "|-------|-----------|------------------|---------------------|------------|------------|--------------|----------|\n",
        "| Epoch[2] | 0.4706 | 84.10% | 84.72% | 87.51% | 83.52% | 79.76% | 88.10% |\n",
        "\n",
        "Note: The categorical names used are: Angry, Happy, Neutral, and Sad.\n",
        "\n",
        "**Test Results:**\n",
        "\n",
        "| Model | Test Loss | Weighted Accuracy | Unweighted Accuracy | Angry Acc. | Happy Acc. | Neutral Acc. | Sad Acc. |\n",
        "|-------|-----------|------------------|---------------------|------------|------------|--------------|----------|\n",
        "| Epoch[2] | 0.6848 | 73.10% | 74.36% | 76.64% | 74.38% | 66.22% | 80.20% |\n",
        "\n",
        "Note: The categorical names used are: Angry, Happy, Neutral, and Sad."
      ],
      "metadata": {
        "id": "8Jm-RGxz9uyn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Saving the Test Results"
      ],
      "metadata": {
        "id": "wNlWChB1ti10"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the pre-trained model\n",
        "main_dir\n",
        "mm_ma_model_path = main_dir + \"model_mm_mla.pth\"\n",
        "pretr_model_mm_mla = torch.load(mm_ma_model_path,map_location=torch.device(device))"
      ],
      "metadata": {
        "id": "T3G3ja_ztinl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# pretr_model_mm_mla = model_mm_mla.to(device)"
      ],
      "metadata": {
        "id": "3ZehOSIAJ90G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_loader = test_dataset_mm"
      ],
      "metadata": {
        "id": "3UIgg13auZwf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set the model to evaluation mode\n",
        "pretr_model_mm_mla.eval()\n",
        "\n",
        "# Create empty lists to store the required information\n",
        "sample_indices = []\n",
        "predicted_labels = []\n",
        "true_labels = []\n",
        "predicted_probs = []\n",
        "z_values = []\n",
        "ll_attention_a_values = []\n",
        "ll_attention_l_values = []\n",
        "# Disable gradient calculation for evaluation\n",
        "with torch.no_grad():\n",
        "    for batch_idx, (feat_a,seq_size_a,feat_l,seq_size_l,labels) in enumerate(test_loader):\n",
        "        feat_a = feat_a.to(device)\n",
        "        feat_l = feat_l.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        # Forward pass through the model\n",
        "        logits, ll_attention_a, ll_attention_l,z = pretr_model_mm_mla(feat_a,seq_size_a, feat_l, seq_size_l)\n",
        "\n",
        "        # Calculate predicted probabilities and labels\n",
        "        probabilities = F.softmax(logits, dim=1)\n",
        "        predictions = torch.argmax(probabilities, dim=1)\n",
        "\n",
        "        # Get the sample indices in the original dataset\n",
        "        original_indices = test_indices[batch_idx * batch_size: (batch_idx + 1) * batch_size]\n",
        "\n",
        "        # Store the required information for each sample in the batch\n",
        "        for idx, label, pred, prob, z_val, ll_a_val, ll_l_val in zip(original_indices, labels, predictions, probabilities, z, ll_attention_a, ll_attention_l):\n",
        "            sample_indices.append(idx)\n",
        "            true_labels.append(label.item())\n",
        "            predicted_labels.append(pred.item())\n",
        "            predicted_probs.append(prob.cpu().numpy())\n",
        "            z_values.append(z_val.cpu().numpy())\n",
        "            ll_attention_a_values.append(ll_a_val.cpu().numpy())\n",
        "            ll_attention_l_values.append(ll_l_val.cpu().numpy())\n"
      ],
      "metadata": {
        "id": "Y0NLhJUot4an"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "audio_paths = []\n",
        "trans_words = []\n",
        "start_stamps = []\n",
        "\n",
        "for idx in sample_indices:\n",
        "    audio_path = data.loc[idx, 'audio_path']\n",
        "    trans_word = data.loc[idx, 'trans_words']\n",
        "    start_stamp = data.loc[idx, 'start_stamp']\n",
        "    \n",
        "    audio_paths.append(audio_path)\n",
        "    trans_words.append(trans_word)\n",
        "    start_stamps.append(start_stamp)"
      ],
      "metadata": {
        "id": "eZiwVa4e1Or1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# get the emotion categories from the interger labels\n",
        "iemocap_dataset.get_encoder().inverse_transform([0, 1, 2,3])\n",
        "predicted_labels_cat = iemocap_dataset.get_encoder().inverse_transform(predicted_labels)\n",
        "true_labels_cat = iemocap_dataset.get_encoder().inverse_transform(true_labels)\n",
        "print(predicted_labels_cat)\n",
        "print(true_labels_cat)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K3AACqR35njs",
        "outputId": "df772ce0-1d60-4caf-b82e-66d18816c200"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['neu' 'neu' 'hap' ... 'neu' 'sad' 'hap']\n",
            "['neu' 'neu' 'neu' ... 'neu' 'sad' 'neu']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# storing the sample sequence lenght of the acoustic and lexical LLDs inputs\n",
        "seq_len_a = []\n",
        "seq_len_l = []\n",
        "for samp in test_dataset_mm:\n",
        "    _,seq_size_a,_,seq_size_l,_ = samp\n",
        "    seq_len_a.append(seq_size_a)\n",
        "    seq_len_l.append(seq_size_l)\n",
        "    # print(seq_size_a,seq_size_l)\n",
        "\n",
        "# removing the padding values from the attention values: \n",
        "for idx in range(analysis_data.shape[0]):\n",
        "    ll_attention_a_values[idx] = ll_attention_a_values[idx].T[0][:seq_len_a[idx]]\n",
        "    ll_attention_l_values[idx] = ll_attention_l_values[idx].T[0][:seq_len_l[idx]]"
      ],
      "metadata": {
        "id": "UAGeWGDRh2T2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "# Create a dictionary from the given lists\n",
        "data_dict = {\n",
        "    'sample_indices': sample_indices,\n",
        "    'predicted_labels_cat': predicted_labels_cat,\n",
        "    'true_labels_cat': true_labels_cat,\n",
        "    'predicted_probs': predicted_probs,\n",
        "    'z_values': z_values,\n",
        "    'll_attention_a_values': ll_attention_a_values,\n",
        "    'll_attention_l_values': ll_attention_l_values,\n",
        "    'audio_paths': audio_paths,\n",
        "    'trans_words': trans_words,\n",
        "    'start_stamps': start_stamps\n",
        "    'seq_len_l':seq_len_l\n",
        "    'seq_len_a':seq_len_a\n",
        "}\n",
        "# Create a pandas data frame from the dictionary\n",
        "analysis_data = pd.DataFrame(data_dict)"
      ],
      "metadata": {
        "id": "9njbLhtGnxVb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Store the data frame in a pickle file\n",
        "with open('analysis_data.pkl', 'wb') as f:\n",
        "    pickle.dump(analysis_data, f)"
      ],
      "metadata": {
        "id": "x16_huXHmTk4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "idx = 7\n",
        "print(\"Sample Index\",sample_indices[idx])\n",
        "print(\"Predicted Label:\",predicted_labels_cat[idx])\n",
        "print(\"True Label:\",true_labels_cat[idx])\n",
        "print(\"trans_words:\",trans_words[idx])\n",
        "print(start_stamps[idx])\n",
        "print(\"Prediction Probablities:\",predicted_probs[idx])\n",
        "print(z_values[idx].T)\n",
        "print(ll_attention_a_values[idx].T)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4rQzGlBu2I8o",
        "outputId": "90d179c9-7691-4642-dda7-6d090a827672"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sample Index 31\n",
            "Predicted Label: hap\n",
            "True Label: hap\n",
            "trans_words: <s> LOOK THERE SEE WHAT'S THAT <sil> NO THAT'S <sil> THAT'S JUST SEAWEED </s>\n",
            "0 4 60 67 73 76 97 106 115 119 136 145\n",
            "Prediction Probablities: [0.3119609  0.6486507  0.03298217 0.00640613]\n",
            "[[7.6825818e-06 1.3120624e-05 2.2176970e-05 ... 5.7989169e-08\n",
            "  5.7989169e-08 5.7989169e-08]]\n",
            "[[1.7885396e-08 1.1439136e-06 5.8797043e-05 5.8951700e-04 2.1982733e-03\n",
            "  5.7809399e-03 7.7269054e-03 8.4549207e-03 2.1782612e-02 4.6532586e-02\n",
            "  9.6331462e-02 1.0548870e-01 2.1491514e-01 2.8426430e-01 1.6519560e-01\n",
            "  3.7971765e-02 2.6908647e-03 1.6449820e-05 1.5342091e-14 1.5342091e-14\n",
            "  1.5342091e-14 1.5342091e-14 1.5342091e-14 1.5342091e-14 1.5342091e-14\n",
            "  1.5342091e-14 1.5342091e-14 1.5342091e-14 1.5342091e-14 1.5342091e-14\n",
            "  1.5342091e-14 1.5342091e-14 1.5342091e-14 1.5342091e-14 1.5342091e-14\n",
            "  1.5342091e-14 1.5342091e-14 1.5342091e-14 1.5342091e-14 1.5342091e-14\n",
            "  1.5342091e-14 1.5342091e-14 1.5342091e-14 1.5342091e-14 1.5342091e-14\n",
            "  1.5342091e-14 1.5342091e-14]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Store the data frame in a pickle file\n",
        "with open('data_frame.pkl', 'wb') as f:\n",
        "    pickle.dump(df, f)"
      ],
      "metadata": {
        "id": "HDK2N47C3CxZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## END"
      ],
      "metadata": {
        "id": "B69rXU0GM7fJ"
      }
    }
  ]
}